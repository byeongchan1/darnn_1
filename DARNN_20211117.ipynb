{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir('/content/gdrive/MyDrive/python/python_dong/data_axis_transform1')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#!pip install torchmetrics\n",
    "import torchmetrics\n",
    "\n",
    "#!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path 지정\n",
    "raw_data_path = './data/stocknet-dataset/price/raw'\n",
    "\n",
    "if 'stocknet' in raw_data_path:\n",
    "    tra_date = '2014-01-02'\n",
    "    val_date = '2015-08-03'\n",
    "    tes_date = '2015-10-01'\n",
    "    end_date = '2015-12-31'\n",
    "elif 'kdd17' in raw_data_path:\n",
    "    tra_date = '2007-01-03'\n",
    "    val_date = '2015-01-02'\n",
    "    tes_date = '2016-01-04'\n",
    "    end_date = '2016-12-31'\n",
    "else:\n",
    "    print('unexpected path: %s' % raw_data_path)\n",
    "\n",
    "# os.path.isfile : 파일이 있는지 없는 지 체크\n",
    "# os.path.join(data_path, fname) : 폴더 디렉터리와 fname(stockname.csv) 붙임\n",
    "fnames = [fname for fname in os.listdir(raw_data_path) if\n",
    "            os.path.isfile(os.path.join(raw_data_path,fname))]\n",
    "\n",
    "COLUMNS_FEATURE_DATA_V1 = ['open_close_ratio', 'high_close_ratio', \n",
    "                           'low_close_ratio', 'close_lastclose_ratio', \n",
    "                           'adjclose_lastadjclose_ratio', 'close_ma5_ratio', \n",
    "                           'close_ma10_ratio', 'close_ma15_ratio', 'close_ma20_ratio', \n",
    "                           'close_ma25_ratio', 'close_ma30_ratio']\n",
    "\n",
    "ver = 'v1' # ver in ['v1', 'v2']\n",
    "if ver == 'v1':\n",
    "    COLUMNS_FEATURE = COLUMNS_FEATURE_DATA_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [5,10,15,20,25,30]\n",
    "\n",
    "def preprocess(df, windows):\n",
    "   '''\n",
    "   전처리 함수 역할 : 전체 feature생성하여 df column에 추가\n",
    "   '''\n",
    "   data = df\n",
    "   data['open_close_ratio'] = data['Open'] / data['Close'] - 1\n",
    "   data['high_close_ratio'] = data['High'] / data['Close'] - 1\n",
    "   data['low_close_ratio'] = data['Low'] / data['Close'] - 1\n",
    "\n",
    "   data['close_lastclose_ratio'] = np.zeros(len(data))\n",
    "   data.loc[1:, 'close_lastclose_ratio'] = data['Close'][1:].values / data['Close'][:-1].values - 1\n",
    "\n",
    "   data['adjclose_lastadjclose_ratio'] = np.zeros(len(data))\n",
    "   data.loc[1:, 'adjclose_lastadjclose_ratio'] = data['Adj Close'][1:].values / data['Adj Close'][:-1].values - 1\n",
    "\n",
    "   for window in windows:\n",
    "      data[f'close_ma{window}_ratio'] = data['Adj Close'].rolling(window).mean()/data['Adj Close'] - 1\n",
    "   \n",
    "   data['label'] = np.append((data['Close'][1:].values > data['Close'][:-1].values)*1,0)\n",
    "\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_path = './data/stocknet-dataset/price/feature'\n",
    "\n",
    "for fname in fnames:\n",
    "   if not os.path.isfile(os.path.join(feature_data_path,fname)):\n",
    "      df_raw = pd.read_csv(os.path.join(raw_data_path,fname))\n",
    "      data = preprocess(df_raw, windows)\n",
    "\n",
    "      # 폴더 없으면 생성\n",
    "      try:\n",
    "         if not os.path.exists(feature_data_path):\n",
    "            os.makedirs(feature_data_path)\n",
    "      except OSError:\n",
    "         print ('Error: Creating directory. ' +  feature_data_path)\n",
    "\n",
    "      #csv 파일 저장\n",
    "      data.to_csv(os.path.join(feature_data_path,fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker : AAPL.csv, date check : True\n",
      "ticker : ABB.csv, date check : True\n",
      "ticker : ABBV.csv, date check : True\n",
      "ticker : AEP.csv, date check : True\n",
      "ticker : AGFS.csv, date check : False\n",
      "ticker : AMGN.csv, date check : True\n",
      "ticker : AMZN.csv, date check : True\n",
      "ticker : BA.csv, date check : True\n",
      "ticker : BABA.csv, date check : False\n",
      "ticker : BAC.csv, date check : True\n",
      "ticker : BBL.csv, date check : True\n",
      "ticker : BCH.csv, date check : True\n",
      "ticker : BHP.csv, date check : True\n",
      "ticker : BP.csv, date check : True\n",
      "ticker : BRK-A.csv, date check : True\n",
      "ticker : BSAC.csv, date check : True\n",
      "ticker : BUD.csv, date check : True\n",
      "ticker : C.csv, date check : True\n",
      "ticker : CAT.csv, date check : True\n",
      "ticker : CELG.csv, date check : True\n",
      "ticker : CHL.csv, date check : True\n",
      "ticker : CHTR.csv, date check : True\n",
      "ticker : CMCSA.csv, date check : True\n",
      "ticker : CODI.csv, date check : True\n",
      "ticker : CSCO.csv, date check : True\n",
      "ticker : CVX.csv, date check : True\n",
      "ticker : D.csv, date check : True\n",
      "ticker : DHR.csv, date check : True\n",
      "ticker : DIS.csv, date check : True\n",
      "ticker : DUK.csv, date check : True\n",
      "ticker : EXC.csv, date check : True\n",
      "ticker : FB.csv, date check : True\n",
      "ticker : GD.csv, date check : True\n",
      "ticker : GE.csv, date check : True\n",
      "ticker : GOOG.csv, date check : True\n",
      "ticker : HD.csv, date check : True\n",
      "ticker : HON.csv, date check : True\n",
      "ticker : HRG.csv, date check : True\n",
      "ticker : HSBC.csv, date check : True\n",
      "ticker : IEP.csv, date check : True\n",
      "ticker : INTC.csv, date check : True\n",
      "ticker : JNJ.csv, date check : True\n",
      "ticker : JPM.csv, date check : True\n",
      "ticker : KO.csv, date check : True\n",
      "ticker : LMT.csv, date check : True\n",
      "ticker : MA.csv, date check : True\n",
      "ticker : MCD.csv, date check : True\n",
      "ticker : MDT.csv, date check : True\n",
      "ticker : MMM.csv, date check : True\n",
      "ticker : MO.csv, date check : True\n",
      "ticker : MRK.csv, date check : True\n",
      "ticker : MSFT.csv, date check : True\n",
      "ticker : NEE.csv, date check : True\n",
      "ticker : NGG.csv, date check : True\n",
      "ticker : NVS.csv, date check : True\n",
      "ticker : ORCL.csv, date check : True\n",
      "ticker : PCG.csv, date check : True\n",
      "ticker : PCLN.csv, date check : True\n",
      "ticker : PEP.csv, date check : True\n",
      "ticker : PFE.csv, date check : True\n",
      "ticker : PG.csv, date check : True\n",
      "ticker : PICO.csv, date check : True\n",
      "ticker : PM.csv, date check : True\n",
      "ticker : PPL.csv, date check : True\n",
      "ticker : PTR.csv, date check : True\n",
      "ticker : RDS-B.csv, date check : True\n",
      "ticker : REX.csv, date check : True\n",
      "ticker : SLB.csv, date check : True\n",
      "ticker : SNP.csv, date check : True\n",
      "ticker : SNY.csv, date check : True\n",
      "ticker : SO.csv, date check : True\n",
      "ticker : SPLP.csv, date check : True\n",
      "ticker : SRE.csv, date check : True\n",
      "ticker : T.csv, date check : True\n",
      "ticker : TM.csv, date check : True\n",
      "ticker : TOT.csv, date check : True\n",
      "ticker : TSM.csv, date check : True\n",
      "ticker : UL.csv, date check : True\n",
      "ticker : UN.csv, date check : True\n",
      "ticker : UNH.csv, date check : True\n",
      "ticker : UPS.csv, date check : True\n",
      "ticker : UTX.csv, date check : True\n",
      "ticker : V.csv, date check : True\n",
      "ticker : VZ.csv, date check : True\n",
      "ticker : WFC.csv, date check : True\n",
      "ticker : WMT.csv, date check : True\n",
      "ticker : XOM.csv, date check : True\n",
      "87 87\n",
      "fail_cnt : 2\n",
      "ticker : SPY.csv, date check : True\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = './data/stocknet-dataset/price/raw'\n",
    "\n",
    "\n",
    "tra_data_X = []\n",
    "tra_data_Y = []\n",
    "val_data_X = []\n",
    "val_data_Y = []\n",
    "test_data_X = []\n",
    "test_data_Y = []\n",
    "tickers = []\n",
    "\n",
    "cnt = 0\n",
    "fail_cnt = 0\n",
    "\n",
    "fnames = [fname for fname in os.listdir(raw_data_path) if\n",
    "            os.path.isfile(os.path.join(raw_data_path,fname))]\n",
    "\n",
    "for fname in fnames:\n",
    "\n",
    "    df = pd.read_csv(os.path.join(raw_data_path,fname))\n",
    "    data = preprocess(df, windows)\n",
    "\n",
    "    learning_data = data[(data['Date'] >= tra_date) & (data['Date'] <= end_date)]['Date']\n",
    "    tra_data_X_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)][COLUMNS_FEATURE]\n",
    "    # tra_data_Y_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)]['adjclose_lastadjclose_ratio']\n",
    "    tra_data_Y_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)]['label']\n",
    "\n",
    "    val_data_X_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)][COLUMNS_FEATURE]\n",
    "    #val_data_Y_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)]['adjclose_lastadjclose_ratio']\n",
    "    val_data_Y_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)]['label'] \n",
    "\n",
    "    test_data_X_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)][COLUMNS_FEATURE]\n",
    "    #test_data_Y_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)]['adjclose_lastadjclose_ratio']\n",
    "    test_data_Y_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)]['label']\n",
    "\n",
    "\n",
    "    if cnt == 0:\n",
    "        target_dates = learning_data\n",
    "    \n",
    "    print('ticker : {}, date check : {}'.format(fname, np.array_equal(target_dates.values, learning_data.values)))\n",
    "    if np.array_equal(target_dates.values, learning_data.values): \n",
    "        \n",
    "        tra_data_X.append(tra_data_X_ticker.values)\n",
    "        tra_data_Y.append(tra_data_Y_ticker.values)\n",
    "\n",
    "        val_data_X.append(val_data_X_ticker.values)\n",
    "        val_data_Y.append(val_data_Y_ticker.values)\n",
    "        \n",
    "        test_data_X.append(test_data_X_ticker.values)\n",
    "        test_data_Y.append(test_data_Y_ticker.values)\n",
    "\n",
    "        tickers.append(fname)\n",
    "    else : \n",
    "        fail_cnt += 1\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "print(cnt, len(fnames))\n",
    "print('fail_cnt :', fail_cnt)\n",
    "\n",
    "# 마지막에 index 종목 넣기\n",
    "raw_data_index_path = './data/stocknet-dataset/price/raw/index'\n",
    "\n",
    "fname = os.listdir(raw_data_index_path)[0]\n",
    "\n",
    "df = pd.read_csv(os.path.join(raw_data_index_path,fname))\n",
    "data = preprocess(df, windows)\n",
    "\n",
    "learning_data = data[(data['Date'] >= tra_date) & (data['Date'] <= end_date)]['Date']\n",
    "tra_data_X_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)][COLUMNS_FEATURE]\n",
    "#tra_data_Y_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)]['adjclose_lastadjclose_ratio']\n",
    "tra_data_Y_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)]['label']\n",
    "\n",
    "val_data_X_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)][COLUMNS_FEATURE]\n",
    "#val_data_Y_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)]['adjclose_lastadjclose_ratio']\n",
    "val_data_Y_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)]['label']\n",
    "\n",
    "test_data_X_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)][COLUMNS_FEATURE]\n",
    "#test_data_Y_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)]['adjclose_lastadjclose_ratio']\n",
    "test_data_Y_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)]['label']\n",
    "\n",
    "print('ticker : {}, date check : {}'.format(fname, np.array_equal(target_dates.values, learning_data.values)))\n",
    "if np.array_equal(target_dates.values, learning_data.values):\n",
    "    \n",
    "    tra_data_X.append(tra_data_X_ticker.values)\n",
    "    # tra_data_Y.append(tra_data_Y_ticker.values)\n",
    "\n",
    "    val_data_X.append(val_data_X_ticker.values)\n",
    "    # val_data_Y.append(val_data_Y_ticker.values)\n",
    "    \n",
    "    test_data_X.append(test_data_X_ticker.values)\n",
    "    # test_data_Y.append(test_data_Y_ticker.values)\n",
    "\n",
    "    tickers.append(fname)\n",
    "\n",
    "# tra_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open_close_ratio               0\n",
       "high_close_ratio               0\n",
       "low_close_ratio                0\n",
       "close_lastclose_ratio          0\n",
       "adjclose_lastadjclose_ratio    0\n",
       "close_ma5_ratio                0\n",
       "close_ma10_ratio               0\n",
       "close_ma15_ratio               0\n",
       "close_ma20_ratio               0\n",
       "close_ma25_ratio               0\n",
       "close_ma30_ratio               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_data_X_ticker.head().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_tensor(list_):\n",
    "    return torch.Tensor(np.array(list_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tra_data_X = list_to_tensor(tra_data_X)\n",
    "tensor_tra_data_Y = list_to_tensor(tra_data_Y).view((len(tra_data_Y),-1,1))\n",
    "tensor_val_data_X = list_to_tensor(val_data_X)\n",
    "tensor_val_data_Y = list_to_tensor(val_data_Y).view((len(val_data_Y),-1,1))\n",
    "tensor_test_data_X = list_to_tensor(test_data_X)\n",
    "tensor_test_data_Y = list_to_tensor(test_data_Y).view((len(test_data_Y),-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"encoder in DA_RNN.\"\"\"\n",
    "\n",
    "    def __init__(self, T,\n",
    "                 input_size,\n",
    "                 encoder_num_hidden,\n",
    "                 parallel=False):\n",
    "        \"\"\"Initialize an encoder in DA_RNN.\"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_num_hidden = encoder_num_hidden\n",
    "        self.input_size = input_size\n",
    "        self.parallel = parallel\n",
    "        self.T = T\n",
    "\n",
    "        # Fig 1. Temporal Attention Mechanism: Encoder is LSTM\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.encoder_num_hidden,\n",
    "            num_layers = 1\n",
    "        )\n",
    "\n",
    "        # Construct Input Attention Mechanism via deterministic attention model\n",
    "        # Eq. 8: W_e[h_{t-1}; s_{t-1}] + U_e * x^k\n",
    "        self.encoder_attn = nn.Linear(\n",
    "            in_features=2 * self.encoder_num_hidden + self.T - 1,\n",
    "            out_features=1\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"forward.\n",
    "\n",
    "        Args:\n",
    "            X: input data\n",
    "\n",
    "        \"\"\"\n",
    "        X_tilde = Variable(X.data.new(\n",
    "            X.size(0), self.T - 1, self.input_size).zero_())\n",
    "        X_encoded = Variable(X.data.new(\n",
    "            X.size(0), self.T - 1, self.encoder_num_hidden).zero_())\n",
    "\n",
    "        # Eq. 8, parameters not in nn.Linear but to be learnt\n",
    "        # v_e = torch.nn.Parameter(data=torch.empty(\n",
    "        #     self.input_size, self.T).uniform_(0, 1), requires_grad=True)\n",
    "        # U_e = torch.nn.Parameter(data=torch.empty(\n",
    "        #     self.T, self.T).uniform_(0, 1), requires_grad=True)\n",
    "\n",
    "        # h_n, s_n: initial states with dimention hidden_size\n",
    "        h_n = self._init_states(X)\n",
    "        s_n = self._init_states(X)\n",
    "\n",
    "        for t in range(self.T - 1):\n",
    "            # batch_size * input_size * (2 * hidden_size + T - 1)\n",
    "            x = torch.cat((h_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
    "                           s_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
    "                           X.permute(0, 2, 1)), dim=2)\n",
    "\n",
    "            x = self.encoder_attn(\n",
    "                x.view(-1, self.encoder_num_hidden * 2 + self.T - 1))\n",
    "\n",
    "            # get weights by softmax\n",
    "            alpha = F.softmax(x.view(-1, self.input_size))\n",
    "\n",
    "            # get new input for LSTM\n",
    "            x_tilde = torch.mul(alpha, X[:, t, :])\n",
    "\n",
    "            # Fix the warning about non-contiguous memory\n",
    "            # https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282\n",
    "            self.encoder_lstm.flatten_parameters()\n",
    "\n",
    "            # encoder LSTM\n",
    "            _, final_state = self.encoder_lstm(x_tilde.unsqueeze(0), (h_n, s_n))\n",
    "            h_n = final_state[0]\n",
    "            s_n = final_state[1]\n",
    "\n",
    "            X_tilde[:, t, :] = x_tilde\n",
    "            X_encoded[:, t, :] = h_n\n",
    "\n",
    "        return X_tilde, X_encoded\n",
    "\n",
    "    def _init_states(self, X):\n",
    "        \"\"\"Initialize all 0 hidden states and cell states for encoder.\n",
    "\n",
    "        Args:\n",
    "            X\n",
    "\n",
    "        Returns:\n",
    "            initial_hidden_states\n",
    "        \"\"\"\n",
    "        # https://pytorch.org/docs/master/nn.html?#lstm\n",
    "        return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_())\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"decoder in DA_RNN.\"\"\"\n",
    "\n",
    "    def __init__(self, T, decoder_num_hidden, encoder_num_hidden):\n",
    "        \"\"\"Initialize a decoder in DA_RNN.\"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_num_hidden = decoder_num_hidden\n",
    "        self.encoder_num_hidden = encoder_num_hidden\n",
    "        self.T = T\n",
    "\n",
    "        self.attn_layer = nn.Sequential(\n",
    "            nn.Linear(2 * decoder_num_hidden + encoder_num_hidden, encoder_num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(encoder_num_hidden, 1)\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=decoder_num_hidden\n",
    "        )\n",
    "        self.fc = nn.Linear(encoder_num_hidden + 1, 1)\n",
    "        self.fc_final = nn.Linear(decoder_num_hidden + encoder_num_hidden, 1)\n",
    "\n",
    "        self.fc.weight.data.normal_()\n",
    "\n",
    "    def forward(self, X_encoded, y_prev):\n",
    "        \"\"\"forward.\"\"\"\n",
    "        d_n = self._init_states(X_encoded)\n",
    "        c_n = self._init_states(X_encoded)\n",
    "\n",
    "        for t in range(self.T - 1):\n",
    "\n",
    "            x = torch.cat((d_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n",
    "                           c_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n",
    "                           X_encoded), dim=2)\n",
    "\n",
    "            beta = F.softmax(self.attn_layer(\n",
    "                x.view(-1, 2 * self.decoder_num_hidden + self.encoder_num_hidden)).view(-1, self.T - 1))\n",
    "\n",
    "            # Eqn. 14: compute context vector\n",
    "            # batch_size * encoder_hidden_size\n",
    "            context = torch.bmm(beta.unsqueeze(1), X_encoded)[:, 0, :]\n",
    "            if t < self.T - 1:\n",
    "                # Eqn. 15\n",
    "                # batch_size * 1\n",
    "                y_tilde = self.fc(\n",
    "                    torch.cat((context, y_prev[:, t].unsqueeze(1)), dim=1))\n",
    "\n",
    "                # Eqn. 16: LSTM\n",
    "                self.lstm_layer.flatten_parameters()\n",
    "                _, final_states = self.lstm_layer(\n",
    "                    y_tilde.unsqueeze(0), (d_n, c_n))\n",
    "\n",
    "                d_n = final_states[0]  # 1 * batch_size * decoder_num_hidden\n",
    "                c_n = final_states[1]  # 1 * batch_size * decoder_num_hidden\n",
    "\n",
    "        # Eqn. 22: final output\n",
    "        y_pred = self.fc_final(torch.cat((d_n[0], context), dim=1))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def _init_states(self, X):\n",
    "        \"\"\"Initialize all 0 hidden states and cell states for encoder.\n",
    "\n",
    "        Args:\n",
    "            X\n",
    "        Returns:\n",
    "            initial_hidden_states\n",
    "\n",
    "        \"\"\"\n",
    "        # hidden state and cell state [num_layers*num_directions, batch_size, hidden_size]\n",
    "        # https://pytorch.org/docs/master/nn.html?#lstm\n",
    "        return Variable(X.data.new(1, X.size(0), self.decoder_num_hidden).zero_())\n",
    "\n",
    "\n",
    "class DA_rnn(nn.Module):\n",
    "    \"\"\"da_rnn.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, T,\n",
    "                 encoder_num_hidden,\n",
    "                 decoder_num_hidden,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 epochs,\n",
    "                 parallel=False):\n",
    "        \"\"\"da_rnn initialization.\"\"\"\n",
    "        super(DA_rnn, self).__init__()\n",
    "        self.encoder_num_hidden = encoder_num_hidden\n",
    "        self.decoder_num_hidden = decoder_num_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.parallel = parallel\n",
    "        self.shuffle = False\n",
    "        self.epochs = epochs\n",
    "        self.T = T\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"==> Use accelerator: \", self.device)\n",
    "\n",
    "        self.Encoder = Encoder(input_size=X.shape[1],\n",
    "                               encoder_num_hidden=encoder_num_hidden,\n",
    "                               T=T).to(self.device)\n",
    "        self.Decoder = Decoder(encoder_num_hidden=encoder_num_hidden,\n",
    "                               decoder_num_hidden=decoder_num_hidden,\n",
    "                               T=T).to(self.device)\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        if self.parallel:\n",
    "            self.encoder = nn.DataParallel(self.encoder)\n",
    "            self.decoder = nn.DataParallel(self.decoder)\n",
    "\n",
    "        self.encoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad,\n",
    "                                                          self.Encoder.parameters()),\n",
    "                                            lr=self.learning_rate)\n",
    "        self.decoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad,\n",
    "                                                          self.Decoder.parameters()),\n",
    "                                            lr=self.learning_rate)\n",
    "\n",
    "        # Training set\n",
    "        self.train_timesteps = int(self.X.shape[0] * 0.7)\n",
    "        self.y = self.y - np.mean(self.y[:self.train_timesteps])\n",
    "        self.input_size = self.X.shape[1]\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"training process.\"\"\"\n",
    "        iter_per_epoch = int(np.ceil(self.train_timesteps * 1. / self.batch_size))\n",
    "        self.iter_losses = np.zeros(self.epochs * iter_per_epoch)\n",
    "        self.epoch_losses = np.zeros(self.epochs)\n",
    "\n",
    "        n_iter = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.shuffle:\n",
    "                ref_idx = np.random.permutation(self.train_timesteps - self.T)\n",
    "            else:\n",
    "                ref_idx = np.array(range(self.train_timesteps - self.T))\n",
    "\n",
    "            idx = 0\n",
    "\n",
    "            while (idx < self.train_timesteps):\n",
    "                # get the indices of X_train\n",
    "                indices = ref_idx[idx:(idx + self.batch_size)]\n",
    "                # x = np.zeros((self.T - 1, len(indices), self.input_size))\n",
    "                x = np.zeros((len(indices), self.T - 1, self.input_size))\n",
    "                y_prev = np.zeros((len(indices), self.T - 1))\n",
    "                y_gt = self.y[indices + self.T]\n",
    "\n",
    "                # format x into 3D tensor\n",
    "                for bs in range(len(indices)):\n",
    "                    x[bs, :, :] = self.X[indices[bs]:(indices[bs] + self.T - 1), :]\n",
    "                    y_prev[bs, :] = self.y[indices[bs]: (indices[bs] + self.T - 1)]\n",
    "\n",
    "                loss = self.train_forward(x, y_prev, y_gt)\n",
    "                self.iter_losses[int(epoch * iter_per_epoch + idx / self.batch_size)] = loss\n",
    "\n",
    "                idx += self.batch_size\n",
    "                n_iter += 1\n",
    "\n",
    "                if n_iter % 10000 == 0 and n_iter != 0:\n",
    "                    for param_group in self.encoder_optimizer.param_groups:\n",
    "                        param_group['lr'] = param_group['lr'] * 0.9\n",
    "                    for param_group in self.decoder_optimizer.param_groups:\n",
    "                        param_group['lr'] = param_group['lr'] * 0.9\n",
    "\n",
    "                self.epoch_losses[epoch] = np.mean(self.iter_losses[range(\n",
    "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epochs: \", epoch, \" Iterations: \", n_iter,\n",
    "                      \" Loss: \", self.epoch_losses[epoch])\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_train_pred = self.test(on_train=True)\n",
    "                y_test_pred = self.test(on_train=False)\n",
    "                y_pred = np.concatenate((y_train_pred, y_test_pred))\n",
    "                plt.ioff()\n",
    "                plt.figure()\n",
    "                plt.plot(range(1, 1 + len(self.y)), self.y, label=\"True\")\n",
    "                plt.plot(range(self.T, len(y_train_pred) + self.T),\n",
    "                         y_train_pred, label='Predicted - Train')\n",
    "                plt.plot(range(self.T + len(y_train_pred), len(self.y) + 1),\n",
    "                         y_test_pred, label='Predicted - Test')\n",
    "                plt.legend(loc='upper left')\n",
    "                plt.show()\n",
    "\n",
    "            # # Save files in last iterations\n",
    "            # if epoch == self.epochs - 1:\n",
    "            #     np.savetxt('../loss.txt', np.array(self.epoch_losses), delimiter=',')\n",
    "            #     np.savetxt('../y_pred.txt',\n",
    "            #                np.array(self.y_pred), delimiter=',')\n",
    "            #     np.savetxt('../y_true.txt',\n",
    "            #                np.array(self.y_true), delimiter=',')\n",
    "\n",
    "    def train_forward(self, X, y_prev, y_gt):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            X:\n",
    "            y_prev:\n",
    "            y_gt: Ground truth label\n",
    "\n",
    "        \"\"\"\n",
    "        # zero gradients\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_weighted, input_encoded = self.Encoder(\n",
    "            Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n",
    "        y_pred = self.Decoder(input_encoded, Variable(\n",
    "            torch.from_numpy(y_prev).type(torch.FloatTensor).to(self.device)))\n",
    "\n",
    "        y_true = Variable(torch.from_numpy(\n",
    "            y_gt).type(torch.FloatTensor).to(self.device))\n",
    "\n",
    "        y_true = y_true.view(-1, 1)\n",
    "        loss = self.criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def test(self, on_train=False):\n",
    "        \"\"\"test.\"\"\"\n",
    "\n",
    "        if on_train:\n",
    "            y_pred = np.zeros(self.train_timesteps - self.T + 1)\n",
    "        else:\n",
    "            y_pred = np.zeros(self.X.shape[0] - self.train_timesteps)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(y_pred):\n",
    "            batch_idx = np.array(range(len(y_pred)))[i: (i + self.batch_size)]\n",
    "            X = np.zeros((len(batch_idx), self.T - 1, self.X.shape[1]))\n",
    "            y_history = np.zeros((len(batch_idx), self.T - 1))\n",
    "\n",
    "            for j in range(len(batch_idx)):\n",
    "                if on_train:\n",
    "                    X[j, :, :] = self.X[range(\n",
    "                        batch_idx[j], batch_idx[j] + self.T - 1), :]\n",
    "                    y_history[j, :] = self.y[range(\n",
    "                        batch_idx[j], batch_idx[j] + self.T - 1)]\n",
    "                else:\n",
    "                    X[j, :, :] = self.X[range(\n",
    "                        batch_idx[j] + self.train_timesteps - self.T, batch_idx[j] + self.train_timesteps - 1), :]\n",
    "                    y_history[j, :] = self.y[range(\n",
    "                        batch_idx[j] + self.train_timesteps - self.T, batch_idx[j] + self.train_timesteps - 1)]\n",
    "\n",
    "            y_history = Variable(torch.from_numpy(\n",
    "                y_history).type(torch.FloatTensor).to(self.device))\n",
    "            _, input_encoded = self.Encoder(\n",
    "                Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n",
    "            y_pred[i:(i + self.batch_size)] = self.Decoder(input_encoded,\n",
    "                                                           y_history).cpu().data.numpy()[:, 0]\n",
    "            i += self.batch_size\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0867f69e5b16c2da0a94f2a308042fc4e0db7f5095985c82e39af55064218631"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('venv36': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
