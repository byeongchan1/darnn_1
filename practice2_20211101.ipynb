{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+sK4LrhLM/1U0bRD8Ye1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byeongchan1/Adv-ALSTM/blob/master/practice2_20211101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbqTdChZm820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9866ffc-0900-4bb4-f2bf-2f2ec06bc60a"
      },
      "source": [
        "# 모듈 import\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('/content/gdrive/MyDrive/python/python_dong/data_axis_transform1')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (2.4.7)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.5.3-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0SHu41LLLR"
      },
      "source": [
        "# 1. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhimcfPoLO8K"
      },
      "source": [
        "# data path 지정\n",
        "raw_data_path = './Adv-ALSTM/data/stocknet-dataset/price/raw'\n",
        "\n",
        "if 'stocknet' in raw_data_path:\n",
        "    tra_date = '2014-01-02'\n",
        "    val_date = '2015-08-03'\n",
        "    tes_date = '2015-10-01'\n",
        "    end_date = '2015-12-31'\n",
        "elif 'kdd17' in raw_data_path:\n",
        "    tra_date = '2007-01-03'\n",
        "    val_date = '2015-01-02'\n",
        "    tes_date = '2016-01-04'\n",
        "    end_date = '2016-12-31'\n",
        "else:\n",
        "    print('unexpected path: %s' % raw_data_path)\n",
        "\n",
        "# os.path.isfile : 파일이 있는지 없는 지 체크\n",
        "# os.path.join(data_path, fname) : 폴더 디렉터리와 fname(stockname.csv) 붙임\n",
        "fnames = [fname for fname in os.listdir(raw_data_path) if\n",
        "            os.path.isfile(os.path.join(raw_data_path,fname))]\n",
        "\n",
        "COLUMNS_FEATURE_DATA_V1 = ['open_close_ratio', 'high_close_ratio', \n",
        "                           'low_close_ratio', 'close_lastclose_ratio', \n",
        "                           'adjclose_lastadjclose_ratio', 'close_ma5_ratio', \n",
        "                           'close_ma10_ratio', 'close_ma15_ratio', 'close_ma20_ratio', \n",
        "                           'close_ma25_ratio', 'close_ma30_ratio']\n",
        "\n",
        "ver = 'v1' # ver in ['v1', 'v2']\n",
        "if ver == 'v1':\n",
        "    COLUMNS_FEATURE = COLUMNS_FEATURE_DATA_V1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HwFoPkbLtaA"
      },
      "source": [
        "windows = [5,10,15,20,25,30]\n",
        "\n",
        "def preprocess(df, windows):\n",
        "   '''\n",
        "   전처리 함수 역할 : 전체 feature생성하여 df column에 추가\n",
        "   '''\n",
        "   data = df\n",
        "   data['open_close_ratio'] = data['Open'] / data['Close'] - 1\n",
        "   data['high_close_ratio'] = data['High'] / data['Close'] - 1\n",
        "   data['low_close_ratio'] = data['Low'] / data['Close'] - 1\n",
        "\n",
        "   data['close_lastclose_ratio'] = np.zeros(len(data))\n",
        "   data.loc[1:, 'close_lastclose_ratio'] = data['Close'][1:].values / data['Close'][:-1].values - 1\n",
        "\n",
        "   data['adjclose_lastadjclose_ratio'] = np.zeros(len(data))\n",
        "   data.loc[1:, 'adjclose_lastadjclose_ratio'] = data['Adj Close'][1:].values / data['Adj Close'][:-1].values - 1\n",
        "\n",
        "   for window in windows:\n",
        "      data[f'close_ma{window}_ratio'] = data['Adj Close'].rolling(window).mean()/data['Adj Close'] - 1\n",
        "   \n",
        "   data['label'] = np.append((data['Close'][1:].values > data['Close'][:-1].values)*1,0)\n",
        "\n",
        "   return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l99aVEg7Lu9X"
      },
      "source": [
        "feature_data_path = './Adv-ALSTM/data/stocknet-dataset/price/feature'\n",
        "\n",
        "for fname in fnames:\n",
        "   if not os.path.isfile(os.path.join(feature_data_path,fname)):\n",
        "      df_raw = pd.read_csv(os.path.join(raw_data_path,fname))\n",
        "      data = preprocess(df_raw, windows)\n",
        "\n",
        "      # 폴더 없으면 생성\n",
        "      try:\n",
        "         if not os.path.exists(feature_data_path):\n",
        "            os.makedirs(feature_data_path)\n",
        "      except OSError:\n",
        "         print ('Error: Creating directory. ' +  feature_data_path)\n",
        "\n",
        "      #csv 파일 저장\n",
        "      data.to_csv(os.path.join(feature_data_path,fname))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmCeDyzQL-f6"
      },
      "source": [
        "# train, validation, test data 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAHggw-cUvYv"
      },
      "source": [
        "## 1. input data 3차원으로 쌓기\n",
        "shape = (stock 종류수, date, feature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrifOTl2U89q"
      },
      "source": [
        "요건\n",
        "1. 모든 티커 데이터의 date가 맞는지?\n",
        "2. 결측치는 없는지?\n",
        "3. 티커와 데이터 메치 가능해야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8iOchcNUzdw",
        "outputId": "054bdd8f-4d85-4741-b1c0-437343fa5be0"
      },
      "source": [
        "raw_data_path = './Adv-ALSTM/data/stocknet-dataset/price/raw'\n",
        "\n",
        "\n",
        "tra_data_X = []\n",
        "tra_data_Y = []\n",
        "val_data_X = []\n",
        "val_data_Y = []\n",
        "test_data_X = []\n",
        "test_data_Y = []\n",
        "tickers = []\n",
        "\n",
        "cnt = 0\n",
        "fail_cnt = 0\n",
        "\n",
        "fnames = [fname for fname in os.listdir(raw_data_path) if\n",
        "            os.path.isfile(os.path.join(raw_data_path,fname))]\n",
        "\n",
        "for fname in fnames:\n",
        "\n",
        "    df = pd.read_csv(os.path.join(raw_data_path,fname))\n",
        "    data = preprocess(df, windows)\n",
        "\n",
        "    learning_data = data[(data['Date'] >= tra_date) & (data['Date'] <= end_date)]['Date']\n",
        "    tra_data_X_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)][COLUMNS_FEATURE]\n",
        "    tra_data_Y_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)]['label']\n",
        "\n",
        "    val_data_X_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)][COLUMNS_FEATURE]\n",
        "    val_data_Y_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)]['label']\n",
        "\n",
        "    test_data_X_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)][COLUMNS_FEATURE]\n",
        "    test_data_Y_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)]['label']\n",
        "\n",
        "\n",
        "    if cnt == 0:\n",
        "        target_dates = learning_data\n",
        "    \n",
        "    print('ticker : {}, date check : {}'.format(fname, np.array_equal(target_dates.values, learning_data.values)))\n",
        "    if np.array_equal(target_dates.values, learning_data.values): \n",
        "        \n",
        "        tra_data_X.append(tra_data_X_ticker.values)\n",
        "        tra_data_Y.append(tra_data_Y_ticker.values)\n",
        "\n",
        "        val_data_X.append(val_data_X_ticker.values)\n",
        "        val_data_Y.append(val_data_Y_ticker.values)\n",
        "        \n",
        "        test_data_X.append(test_data_X_ticker.values)\n",
        "        test_data_Y.append(test_data_Y_ticker.values)\n",
        "\n",
        "        tickers.append(fname)\n",
        "    else : \n",
        "        fail_cnt += 1\n",
        "    \n",
        "    cnt += 1\n",
        "\n",
        "print(cnt, len(fnames))\n",
        "print('fail_cnt :', fail_cnt)\n",
        "\n",
        "# 마지막에 index 종목 넣기\n",
        "raw_data_index_path = './Adv-ALSTM/data/stocknet-dataset/price/raw/index'\n",
        "\n",
        "fname = os.listdir(raw_data_index_path)[0]\n",
        "\n",
        "df = pd.read_csv(os.path.join(raw_data_index_path,fname))\n",
        "data = preprocess(df, windows)\n",
        "\n",
        "learning_data = data[(data['Date'] >= tra_date) & (data['Date'] <= end_date)]['Date']\n",
        "tra_data_X_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)][COLUMNS_FEATURE]\n",
        "tra_data_Y_ticker = data[(data['Date'] >= tra_date) & (data['Date'] < val_date)]['label']\n",
        "\n",
        "val_data_X_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)][COLUMNS_FEATURE]\n",
        "val_data_Y_ticker = data[(data['Date'] >= val_date) & (data['Date'] < tes_date)]['label']\n",
        "\n",
        "test_data_X_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)][COLUMNS_FEATURE]\n",
        "test_data_Y_ticker = data[(data['Date'] >= tes_date) & (data['Date'] <= end_date)]['label']\n",
        "\n",
        "print('ticker : {}, date check : {}'.format(fname, np.array_equal(target_dates.values, learning_data.values)))\n",
        "if np.array_equal(target_dates.values, learning_data.values):\n",
        "    \n",
        "    tra_data_X.append(tra_data_X_ticker.values)\n",
        "    # tra_data_Y.append(tra_data_Y_ticker.values)\n",
        "\n",
        "    val_data_X.append(val_data_X_ticker.values)\n",
        "    # val_data_Y.append(val_data_Y_ticker.values)\n",
        "    \n",
        "    test_data_X.append(test_data_X_ticker.values)\n",
        "    # test_data_Y.append(test_data_Y_ticker.values)\n",
        "\n",
        "    tickers.append(fname)\n",
        "\n",
        "# tra_data_X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ticker : SPLP.csv, date check : True\n",
            "ticker : CELG.csv, date check : True\n",
            "ticker : WFC.csv, date check : True\n",
            "ticker : INTC.csv, date check : True\n",
            "ticker : JNJ.csv, date check : True\n",
            "ticker : AAPL.csv, date check : True\n",
            "ticker : GOOG.csv, date check : True\n",
            "ticker : BP.csv, date check : True\n",
            "ticker : GE.csv, date check : True\n",
            "ticker : BABA.csv, date check : False\n",
            "ticker : PTR.csv, date check : True\n",
            "ticker : AMZN.csv, date check : True\n",
            "ticker : SLB.csv, date check : True\n",
            "ticker : SRE.csv, date check : True\n",
            "ticker : KO.csv, date check : True\n",
            "ticker : UTX.csv, date check : True\n",
            "ticker : BHP.csv, date check : True\n",
            "ticker : BRK-A.csv, date check : True\n",
            "ticker : NEE.csv, date check : True\n",
            "ticker : DHR.csv, date check : True\n",
            "ticker : BBL.csv, date check : True\n",
            "ticker : CVX.csv, date check : True\n",
            "ticker : NVS.csv, date check : True\n",
            "ticker : CAT.csv, date check : True\n",
            "ticker : PICO.csv, date check : True\n",
            "ticker : VZ.csv, date check : True\n",
            "ticker : AEP.csv, date check : True\n",
            "ticker : T.csv, date check : True\n",
            "ticker : HD.csv, date check : True\n",
            "ticker : PG.csv, date check : True\n",
            "ticker : BCH.csv, date check : True\n",
            "ticker : WMT.csv, date check : True\n",
            "ticker : SNY.csv, date check : True\n",
            "ticker : HSBC.csv, date check : True\n",
            "ticker : AMGN.csv, date check : True\n",
            "ticker : UPS.csv, date check : True\n",
            "ticker : NGG.csv, date check : True\n",
            "ticker : BA.csv, date check : True\n",
            "ticker : MA.csv, date check : True\n",
            "ticker : IEP.csv, date check : True\n",
            "ticker : XOM.csv, date check : True\n",
            "ticker : BSAC.csv, date check : True\n",
            "ticker : DIS.csv, date check : True\n",
            "ticker : BUD.csv, date check : True\n",
            "ticker : PPL.csv, date check : True\n",
            "ticker : ABB.csv, date check : True\n",
            "ticker : CHL.csv, date check : True\n",
            "ticker : AGFS.csv, date check : False\n",
            "ticker : SO.csv, date check : True\n",
            "ticker : GD.csv, date check : True\n",
            "ticker : TSM.csv, date check : True\n",
            "ticker : PM.csv, date check : True\n",
            "ticker : FB.csv, date check : True\n",
            "ticker : DUK.csv, date check : True\n",
            "ticker : CHTR.csv, date check : True\n",
            "ticker : MO.csv, date check : True\n",
            "ticker : PFE.csv, date check : True\n",
            "ticker : MSFT.csv, date check : True\n",
            "ticker : D.csv, date check : True\n",
            "ticker : HON.csv, date check : True\n",
            "ticker : UNH.csv, date check : True\n",
            "ticker : TOT.csv, date check : True\n",
            "ticker : UN.csv, date check : True\n",
            "ticker : V.csv, date check : True\n",
            "ticker : HRG.csv, date check : True\n",
            "ticker : EXC.csv, date check : True\n",
            "ticker : CSCO.csv, date check : True\n",
            "ticker : LMT.csv, date check : True\n",
            "ticker : MCD.csv, date check : True\n",
            "ticker : BAC.csv, date check : True\n",
            "ticker : RDS-B.csv, date check : True\n",
            "ticker : PCG.csv, date check : True\n",
            "ticker : UL.csv, date check : True\n",
            "ticker : C.csv, date check : True\n",
            "ticker : CODI.csv, date check : True\n",
            "ticker : SNP.csv, date check : True\n",
            "ticker : ORCL.csv, date check : True\n",
            "ticker : PEP.csv, date check : True\n",
            "ticker : PCLN.csv, date check : True\n",
            "ticker : MDT.csv, date check : True\n",
            "ticker : TM.csv, date check : True\n",
            "ticker : ABBV.csv, date check : True\n",
            "ticker : MMM.csv, date check : True\n",
            "ticker : JPM.csv, date check : True\n",
            "ticker : REX.csv, date check : True\n",
            "ticker : CMCSA.csv, date check : True\n",
            "ticker : MRK.csv, date check : True\n",
            "87 87\n",
            "fail_cnt : 2\n",
            "ticker : SPY.csv, date check : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YEV0_P9VLEi"
      },
      "source": [
        "def list_to_tensor(list_):\n",
        "    return torch.Tensor(np.array(list_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9DYt4VFVZcD"
      },
      "source": [
        "tensor_tra_data_X = list_to_tensor(tra_data_X)\n",
        "tensor_tra_data_Y = list_to_tensor(tra_data_Y).view((len(tra_data_Y),-1,1))\n",
        "tensor_val_data_X = list_to_tensor(val_data_X)\n",
        "tensor_val_data_Y = list_to_tensor(val_data_Y).view((len(val_data_Y),-1,1))\n",
        "tensor_test_data_X = list_to_tensor(test_data_X)\n",
        "tensor_test_data_Y = list_to_tensor(test_data_Y).view((len(test_data_Y),-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMzjM7VVdX2"
      },
      "source": [
        "## Hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNe5DtYMVb5S"
      },
      "source": [
        "w = 10 # window size w in {10, 15}\n",
        "beta = 0.01 # market context weight beta in {0.01, 0.1, 1}\n",
        "h = 64 # hidden layer size h in {64, 128}\n",
        "learning_rate = 0.001 # in {0.001, 0.0001}\n",
        "lambda_1 = 1 # selective regularzation lambda = 1\n",
        "drop_rate = 0.15\n",
        "feature_size = len(COLUMNS_FEATURE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiKtVSNCbtiM"
      },
      "source": [
        "class Feature_transformation_layer(nn.Module):\n",
        "    \"\"\"Feature transformation layer in DTML\"\"\"\n",
        "\n",
        "    def __init__(self, feature_size, h):\n",
        "        super(Feature_transformation_layer, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.h = h\n",
        "\n",
        "        self.linear = nn.Linear(self.feature_size, self.h)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        output = torch.tanh(self.linear(X))\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-BY7qcfaEMu"
      },
      "source": [
        "def layer_normalization(tensor):\n",
        "    h = tensor.size()[-1]\n",
        "    return (tensor - torch.std_mean(tensor, dim=1, unbiased=False)[1].repeat((h,1)).transpose(-2,-1))/torch.std_mean(tensor, dim=1, unbiased=False)[0].repeat((h,1)).transpose(-2,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt9mLCHDY2Y0"
      },
      "source": [
        "class Nonlinear_transformation(nn.Module):\n",
        "    \"\"\"Nonlinear transformation in DTML\"\"\"\n",
        "\n",
        "    def __init__(self, h):\n",
        "        super(Nonlinear_transformation, self).__init__()\n",
        "        self.h = h\n",
        "\n",
        "\n",
        "        self.mlp1 = nn.Linear(self.h, self.h * 4)\n",
        "        self.mlp2 = nn.Linear(self.h * 4, h)\n",
        "    \n",
        "    def forward(self, H, H_tilda):\n",
        "        H_p = H + H_tilda\n",
        "        H_p = self.mlp1(H_p)\n",
        "        H_p = nn.ReLU()(H_p)\n",
        "        H_p = self.mlp2(H_p)\n",
        "        H_p = H + H_tilda + H_p\n",
        "        H_p = nn.Tanh()(H_p)\n",
        "\n",
        "        # dropout and layer normalization\n",
        "        H_p = nn.Dropout(p=drop_rate)(H_p)\n",
        "        H_p = layer_normalization(H_p)\n",
        "        return H_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuXwwHOabisN"
      },
      "source": [
        "class Final_prediction(nn.Module):\n",
        "    \"\"\"final prediction in DTML\"\"\"\n",
        "\n",
        "    def __init__(self, h):\n",
        "        super(Final_prediction, self).__init__()\n",
        "        self.h = h\n",
        "\n",
        "        self.pred_linear = nn.Linear(h,1)\n",
        "    \n",
        "    def forward(self, H_p):\n",
        "        y_hat = self.pred_linear(H_p)\n",
        "        y_hat = nn.Sigmoid()(y_hat).view((-1))\n",
        "\n",
        "        return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAZmRDLhVn_j"
      },
      "source": [
        "class DTML(nn.Module):\n",
        "    ''' DTML '''\n",
        "\n",
        "    def __init__(self, X, y, w, h, beta, test_X, test_y,\n",
        "                 batch_size,\n",
        "                 learning_rate, epochs):\n",
        "        \"\"\" Initialize \"\"\"\n",
        "        super(DTML, self).__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.test_X = test_X\n",
        "        self.test_y = test_y\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "        self.beta = beta\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.train_timesteps = train_timesteps = int(X.size()[1]) - w\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        print(\"==> Use accelerator : \", self.device)\n",
        "\n",
        "        self.Feature_transformation_layer = Feature_transformation_layer(feature_size=self.X.size()[-1],\n",
        "                                                                         h=self.h)\n",
        "        \n",
        "        # 티커별로 lstm 층 개별로 만들기\n",
        "        for i in range(self.X.size()[0]): # 마지막은 index용 lstm\n",
        "            self._modules['lstm_{}'.format(i)] = nn.LSTM(input_size = self.h, hidden_size=self.h, batch_first=True)\n",
        "        \n",
        "        # ticker별로 linear layer 생성\n",
        "        for i in range(self.X.size()[0]):\n",
        "            self._modules['ContextNormalLinearLayer_{}'.format(i)] = nn.Linear(h, h)\n",
        "\n",
        "        self.query_layer = nn.Linear(self.h, self.h)\n",
        "        self.key_layer = nn.Linear(self.h, self.h)\n",
        "        self.value_layer = nn.Linear(self.h, self.h)\n",
        "\n",
        "        self.Nonlinear_transformation = Nonlinear_transformation(h=self.h)\n",
        "\n",
        "        self.Final_prediction = Final_prediction(h=self.h)\n",
        "\n",
        "        # Loss funciton\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "        self.optimizer = optim.Adam(params=self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def window_(self, tensor, time_idx, window_size):\n",
        "        # tensor size = (ticker_number, dates, feature_size)    \n",
        "        return tensor[:, time_idx:time_idx + self.w, :]\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"training process\"\"\"\n",
        "        # epoch당 iteration 수\n",
        "        iter_per_epoch = int(np.ceil(self.train_timesteps * 1. / self.batch_size))\n",
        "        # 전체 iteration에서 loss (전체 iteration 수 = epoch수 * epoch당 iteration 수)\n",
        "        self.iter_losses = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.iter_acc = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.iter_mcc = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.iter_precision = np.zeros(self.epochs * iter_per_epoch)\n",
        "        # epoch당 loss\n",
        "        self.epoch_losses = np.zeros(self.epochs)\n",
        "        self.epoch_acc = np.zeros(self.epochs)\n",
        "        self.epoch_mcc = np.zeros(self.epochs)\n",
        "        self.epoch_precision = np.zeros(self.epochs)\n",
        "\n",
        "        n_iter = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            time_idx = 0\n",
        "            output = self.Feature_transformation_layer(self.X)\n",
        "\n",
        "            while (time_idx < self.train_timesteps):\n",
        "                #print('time_idx : {}, train_timesteps : {}'.format(time_idx, self.train_timesteps))\n",
        "                \n",
        "                Z_tilda = self.window_(output, time_idx=time_idx, window_size=self.w)\n",
        "                tra_data_Y_w = self.window_(self.y, time_idx=time_idx, window_size=self.w)\n",
        "\n",
        "                # hidden state ticker별 쌓기\n",
        "                H_n = torch.Tensor().new_zeros((self.X.size()[0],w,h)) # +1은 index\n",
        "\n",
        "                for ticker_idx in range(self.X.size()[0]):\n",
        "                    # print('ticker_idx', ticker_idx)\n",
        "                    i = 0\n",
        "                    z_tilda = Z_tilda[ticker_idx,:,:].view(1,Z_tilda.size()[1], Z_tilda.size()[2])\n",
        "                    lstm_output, (h_n, c_n) = self._modules['lstm_{}'.format(ticker_idx)](z_tilda[:,i,:].view(1,1,Z_tilda.size()[2]))\n",
        "                    H_n[ticker_idx,0,:] = h_n\n",
        "                    for i in range(1,w):\n",
        "                        lstm_output, (h_n, c_n) = self._modules['lstm_{}'.format(ticker_idx)](z_tilda[:,i,:].view(1,1,Z_tilda.size()[2]), (h_n, c_n))\n",
        "                        H_n[ticker_idx,i,:] = h_n\n",
        "\n",
        "                H_n_dot = torch.Tensor().new_zeros((self.X.size()[0],self.w)) # 알파_i 계산하기전 ticker별로 dot(h_i, h_T) 계산\n",
        "                # size (len(tickers), window_size w)\n",
        "                # https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "                # torch.mul broadcasting 볼수 있음\n",
        "                for i in range(self.X.size()[0]):\n",
        "                    H_n_dot[i,:] = torch.matmul(H_n[i], H_n[i][-1])\n",
        "\n",
        "                # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "                Alpha = nn.Softmax(dim=1)(torch.exp(H_n_dot))\n",
        "\n",
        "                H_n_tilde = torch.Tensor().new_zeros((self.X.size()[0], self.h))\n",
        "                for i in range(self.X.size()[0]):\n",
        "                    H_n_tilde[i,:] = torch.matmul(H_n[i].transpose(-2,-1), Alpha[i])\n",
        "\n",
        "                pre_H_c_n = layer_normalization(H_n_tilde)\n",
        "\n",
        "                H_c_n = torch.Tensor().new_zeros(self.X.size()[0], self.h)\n",
        "                for i in range(self.X.size()[0]):\n",
        "                    H_c_n[i,:]= self._modules['ContextNormalLinearLayer_{}'.format(i)](pre_H_c_n[i])\n",
        "\n",
        "                H = H_c_n + self.beta*H_c_n[-1]\n",
        "                H = H[:-1] # 마지막 index context vector 제외\n",
        "\n",
        "                Q = self.query_layer(H)\n",
        "                K = self.key_layer(H)\n",
        "                V = self.value_layer(H)\n",
        "\n",
        "                # S 생성\n",
        "                S = torch.matmul(Q, K.transpose(-2,-1))\n",
        "                S = S/math.sqrt(h)\n",
        "                S = nn.Softmax(dim=1)(S)\n",
        "\n",
        "                # H_tilda 생성\n",
        "                H_tilda = torch.matmul(S,V)\n",
        "\n",
        "                # dropout and layer normailzation\n",
        "                H_tilda = nn.Dropout(p=drop_rate)(H_tilda)\n",
        "                H_tilda = layer_normalization(H_tilda)\n",
        "\n",
        "                H_p = self.Nonlinear_transformation(H, H_tilda)\n",
        "\n",
        "                y_hat = self.Final_prediction(H_p)\n",
        "                y = tra_data_Y_w[:,w-1,:].view((-1))\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                ## Selective Regularization\n",
        "                loss_reg = self.criterion(y_hat,y) + lambda_1*(torch.norm(list(self.Final_prediction.parameters())[0]).pow(2)\n",
        "                                 + torch.norm(list(self.Final_prediction.parameters())[1]).pow(2))\n",
        "                \n",
        "                acc = torchmetrics.functional.accuracy(y_hat,y.int())\n",
        "                mcc = torchmetrics.MatthewsCorrcoef(num_classes=2)(y_hat, y.int())\n",
        "                precision = torchmetrics.Precision()(y_hat, y.int())\n",
        "\n",
        "                loss_reg.backward(retain_graph=True)\n",
        "                self.optimizer.step()\n",
        "              \n",
        "\n",
        "                self.iter_losses[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = loss_reg\n",
        "                self.iter_acc[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = acc\n",
        "                self.iter_mcc[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = mcc\n",
        "                self.iter_precision[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = precision\n",
        "                if time_idx % 10 == 0:\n",
        "                    print('epoch : ',epoch, \"time_idx : \", time_idx, \n",
        "                          \"loss_reg : \", loss_reg, \n",
        "                          \"acc : \", acc, \n",
        "                          \"mcc : \", mcc,\n",
        "                          \"precision : \", precision)\n",
        "\n",
        "                time_idx += self.batch_size\n",
        "                n_iter += 1\n",
        "\n",
        "                self.epoch_losses[epoch] = np.mean(self.iter_losses[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "                \n",
        "                self.epoch_acc[epoch] = np.mean(self.iter_acc[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "                \n",
        "                self.epoch_mcc[epoch] = np.mean(self.iter_mcc[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "                \n",
        "                self.epoch_precision[epoch] = np.mean(self.iter_precision[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epochs : \", epoch, \"Iterations : \", n_iter,\n",
        "                      \"Loss : \", self.epoch_losses[epoch],\n",
        "                      \"ACC : \", self.epoch_acc[epoch],\n",
        "                      \"MCC : \", self.epoch_mcc[epoch],\n",
        "                      \"Precision : \", self.epoch_precision[epoch])\n",
        "    \n",
        "    def test(self):\n",
        "        # epoch당 iteration 수\n",
        "        iter_per_epoch = int(np.ceil(self.train_timesteps * 1. / self.batch_size))\n",
        "        # 전체 iteration에서 loss (전체 iteration 수 = epoch수 * epoch당 iteration 수)\n",
        "        self.iter_losses = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.iter_acc = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.iter_mcc = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.iter_precision = np.zeros(self.epochs * iter_per_epoch)\n",
        "        # epoch당 loss\n",
        "        self.epoch_losses = np.zeros(self.epochs)\n",
        "        self.epoch_acc = np.zeros(self.epochs)\n",
        "        self.epoch_mcc = np.zeros(self.epochs)\n",
        "        self.epoch_precision = np.zeros(self.epochs)\n",
        "\n",
        "        n_iter = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            time_idx = 0\n",
        "            output = self.Feature_transformation_layer(self.X)\n",
        "\n",
        "            while (time_idx < self.train_timesteps): # test끝으로 바꿔야함\n",
        "                #print('time_idx : {}, train_timesteps : {}'.format(time_idx, self.train_timesteps))\n",
        "                \n",
        "                Z_tilda = self.window_(output, time_idx=time_idx, window_size=self.w)\n",
        "                tra_data_Y_w = self.window_(self.y, time_idx=time_idx, window_size=self.w)\n",
        "\n",
        "                # hidden state ticker별 쌓기\n",
        "                H_n = torch.Tensor().new_zeros((self.X.size()[0],w,h)) # +1은 index\n",
        "\n",
        "                for ticker_idx in range(self.X.size()[0]):\n",
        "                    # print('ticker_idx', ticker_idx)\n",
        "                    i = 0\n",
        "                    z_tilda = Z_tilda[ticker_idx,:,:].view(1,Z_tilda.size()[1], Z_tilda.size()[2])\n",
        "                    lstm_output, (h_n, c_n) = self._modules['lstm_{}'.format(ticker_idx)](z_tilda[:,i,:].view(1,1,Z_tilda.size()[2]))\n",
        "                    H_n[ticker_idx,0,:] = h_n\n",
        "                    for i in range(1,w):\n",
        "                        lstm_output, (h_n, c_n) = self._modules['lstm_{}'.format(ticker_idx)](z_tilda[:,i,:].view(1,1,Z_tilda.size()[2]), (h_n, c_n))\n",
        "                        H_n[ticker_idx,i,:] = h_n\n",
        "\n",
        "                H_n_dot = torch.Tensor().new_zeros((self.X.size()[0],self.w)) # 알파_i 계산하기전 ticker별로 dot(h_i, h_T) 계산\n",
        "                # size (len(tickers), window_size w)\n",
        "                # https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "                # torch.mul broadcasting 볼수 있음\n",
        "                for i in range(self.X.size()[0]):\n",
        "                    H_n_dot[i,:] = torch.matmul(H_n[i], H_n[i][-1])\n",
        "\n",
        "                # https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "                Alpha = nn.Softmax(dim=1)(torch.exp(H_n_dot))\n",
        "\n",
        "                H_n_tilde = torch.Tensor().new_zeros((self.X.size()[0], self.h))\n",
        "                for i in range(self.X.size()[0]):\n",
        "                    H_n_tilde[i,:] = torch.matmul(H_n[i].transpose(-2,-1), Alpha[i])\n",
        "\n",
        "                pre_H_c_n = layer_normalization(H_n_tilde)\n",
        "\n",
        "                H_c_n = torch.Tensor().new_zeros(self.X.size()[0], self.h)\n",
        "                for i in range(self.X.size()[0]):\n",
        "                    H_c_n[i,:]= self._modules['ContextNormalLinearLayer_{}'.format(i)](pre_H_c_n[i])\n",
        "\n",
        "                H = H_c_n + self.beta*H_c_n[-1]\n",
        "                H = H[:-1] # 마지막 index context vector 제외\n",
        "\n",
        "                Q = self.query_layer(H)\n",
        "                K = self.key_layer(H)\n",
        "                V = self.value_layer(H)\n",
        "\n",
        "                # S 생성\n",
        "                S = torch.matmul(Q, K.transpose(-2,-1))\n",
        "                S = S/math.sqrt(h)\n",
        "                S = nn.Softmax(dim=1)(S)\n",
        "\n",
        "                # H_tilda 생성\n",
        "                H_tilda = torch.matmul(S,V)\n",
        "\n",
        "                # dropout and layer normailzation\n",
        "                H_tilda = nn.Dropout(p=drop_rate)(H_tilda)\n",
        "                H_tilda = layer_normalization(H_tilda)\n",
        "\n",
        "                H_p = self.Nonlinear_transformation(H, H_tilda)\n",
        "\n",
        "                y_hat = self.Final_prediction(H_p)\n",
        "                y = tra_data_Y_w[:,w-1,:].view((-1))\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                ## Selective Regularization\n",
        "                loss_reg = self.criterion(y_hat,y) + lambda_1*(torch.norm(list(self.Final_prediction.parameters())[0]).pow(2)\n",
        "                                 + torch.norm(list(self.Final_prediction.parameters())[1]).pow(2))\n",
        "                \n",
        "                acc = torchmetrics.functional.accuracy(y_hat,y.int())\n",
        "                mcc = torchmetrics.MatthewsCorrcoef(num_classes=2)(y_hat, y.int())\n",
        "                precision = torchmetrics.Precision()(y_hat, y.int())\n",
        "\n",
        "                loss_reg.backward(retain_graph=True)\n",
        "                self.optimizer.step()\n",
        "              \n",
        "\n",
        "                self.iter_losses[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = loss_reg\n",
        "                self.iter_acc[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = acc\n",
        "                self.iter_mcc[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = mcc\n",
        "                self.iter_precision[int(epoch * iter_per_epoch + time_idx / self.batch_size)] = precision\n",
        "                if time_idx % 10 == 0:\n",
        "                    print('epoch : ',epoch, \"time_idx : \", time_idx, \n",
        "                          \"loss_reg : \", loss_reg, \n",
        "                          \"acc : \", acc, \n",
        "                          \"mcc : \", mcc,\n",
        "                          \"precision : \", precision)\n",
        "\n",
        "                time_idx += self.batch_size\n",
        "                n_iter += 1\n",
        "\n",
        "                self.epoch_losses[epoch] = np.mean(self.iter_losses[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "                \n",
        "                self.epoch_acc[epoch] = np.mean(self.iter_acc[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "                \n",
        "                self.epoch_mcc[epoch] = np.mean(self.iter_mcc[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "                \n",
        "                self.epoch_precision[epoch] = np.mean(self.iter_precision[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epochs : \", epoch, \"Iterations : \", n_iter,\n",
        "                      \"Loss : \", self.epoch_losses[epoch],\n",
        "                      \"ACC : \", self.epoch_acc[epoch],\n",
        "                      \"MCC : \", self.epoch_mcc[epoch],\n",
        "                      \"Precision : \", self.epoch_precision[epoch])\n",
        "\n",
        "\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcviXziCYLdK",
        "outputId": "4f874183-e283-44b9-a12a-a2176b791f21"
      },
      "source": [
        "model = DTML(\n",
        "    X=tensor_tra_data_X,\n",
        "    y=tensor_tra_data_Y,\n",
        "     w=w, h=h, batch_size=1, beta = 0.01,\n",
        "     learning_rate=learning_rate, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Use accelerator :  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bE_RLJ6bfzN",
        "outputId": "60e5277a-757f-4835-9fa7-aad7cca7e138"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  0 time_idx :  0 loss_reg :  tensor(0.9800, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(-0.0182) precision :  tensor(0.4286)\n",
            "epoch :  0 time_idx :  10 loss_reg :  tensor(0.7893, grad_fn=<AddBackward0>) acc :  tensor(0.7412) mcc :  tensor(0.1518) precision :  tensor(0.5000)\n",
            "epoch :  0 time_idx :  20 loss_reg :  tensor(0.8372, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.0862) precision :  tensor(0.8491)\n",
            "epoch :  0 time_idx :  30 loss_reg :  tensor(1.1048, grad_fn=<AddBackward0>) acc :  tensor(0.1294) mcc :  tensor(nan) precision :  tensor(0.1294)\n",
            "epoch :  0 time_idx :  40 loss_reg :  tensor(0.7752, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(0.0758) precision :  tensor(0.9194)\n",
            "epoch :  0 time_idx :  50 loss_reg :  tensor(0.7943, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.2243) precision :  tensor(0.8810)\n",
            "epoch :  0 time_idx :  60 loss_reg :  tensor(0.7945, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(-0.1115) precision :  tensor(0.8222)\n",
            "epoch :  0 time_idx :  70 loss_reg :  tensor(0.7859, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(-0.1377) precision :  tensor(0.5758)\n",
            "epoch :  0 time_idx :  80 loss_reg :  tensor(0.7644, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0513) precision :  tensor(0.5652)\n",
            "epoch :  0 time_idx :  90 loss_reg :  tensor(0.7779, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.1424) precision :  tensor(0.4143)\n",
            "epoch :  0 time_idx :  100 loss_reg :  tensor(0.8194, grad_fn=<AddBackward0>) acc :  tensor(0.2235) mcc :  tensor(-0.0747) precision :  tensor(0.1795)\n",
            "epoch :  0 time_idx :  110 loss_reg :  tensor(0.7112, grad_fn=<AddBackward0>) acc :  tensor(0.6471) mcc :  tensor(-0.0205) precision :  tensor(0.6795)\n",
            "epoch :  0 time_idx :  120 loss_reg :  tensor(0.7533, grad_fn=<AddBackward0>) acc :  tensor(0.3176) mcc :  tensor(-0.0511) precision :  tensor(0.3000)\n",
            "epoch :  0 time_idx :  130 loss_reg :  tensor(0.7206, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(-0.0373) precision :  tensor(0.6250)\n",
            "epoch :  0 time_idx :  140 loss_reg :  tensor(0.7089, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.0430) precision :  tensor(0.3636)\n",
            "epoch :  0 time_idx :  150 loss_reg :  tensor(0.7237, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.0440) precision :  tensor(0.6154)\n",
            "epoch :  0 time_idx :  160 loss_reg :  tensor(0.6897, grad_fn=<AddBackward0>) acc :  tensor(0.6941) mcc :  tensor(0.0312) precision :  tensor(0.7215)\n",
            "epoch :  0 time_idx :  170 loss_reg :  tensor(0.7117, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.1890) precision :  tensor(0.5122)\n",
            "epoch :  0 time_idx :  180 loss_reg :  tensor(0.7010, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.1195) precision :  tensor(0.8772)\n",
            "epoch :  0 time_idx :  190 loss_reg :  tensor(0.7628, grad_fn=<AddBackward0>) acc :  tensor(0.1059) mcc :  tensor(0.0352) precision :  tensor(1.)\n",
            "epoch :  0 time_idx :  200 loss_reg :  tensor(0.7154, grad_fn=<AddBackward0>) acc :  tensor(0.3059) mcc :  tensor(0.0838) precision :  tensor(0.9500)\n",
            "epoch :  0 time_idx :  210 loss_reg :  tensor(0.7144, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(nan) precision :  tensor(0.4588)\n",
            "epoch :  0 time_idx :  220 loss_reg :  tensor(0.7571, grad_fn=<AddBackward0>) acc :  tensor(0.2706) mcc :  tensor(nan) precision :  tensor(0.2706)\n",
            "epoch :  0 time_idx :  230 loss_reg :  tensor(0.7456, grad_fn=<AddBackward0>) acc :  tensor(0.1765) mcc :  tensor(nan) precision :  tensor(0.1765)\n",
            "epoch :  0 time_idx :  240 loss_reg :  tensor(0.7217, grad_fn=<AddBackward0>) acc :  tensor(0.1765) mcc :  tensor(0.0913) precision :  tensor(0.1250)\n",
            "epoch :  0 time_idx :  250 loss_reg :  tensor(0.6859, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(0.1557) precision :  tensor(1.)\n",
            "epoch :  0 time_idx :  260 loss_reg :  tensor(0.7164, grad_fn=<AddBackward0>) acc :  tensor(0.1882) mcc :  tensor(0.0689) precision :  tensor(1.)\n",
            "epoch :  0 time_idx :  270 loss_reg :  tensor(0.7023, grad_fn=<AddBackward0>) acc :  tensor(0.2941) mcc :  tensor(-0.2285) precision :  tensor(0.6923)\n",
            "epoch :  0 time_idx :  280 loss_reg :  tensor(0.7098, grad_fn=<AddBackward0>) acc :  tensor(0.3412) mcc :  tensor(-0.2049) precision :  tensor(0.3494)\n",
            "epoch :  0 time_idx :  290 loss_reg :  tensor(0.6962, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0363) precision :  tensor(0.1905)\n",
            "epoch :  0 time_idx :  300 loss_reg :  tensor(0.7004, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(0.0891) precision :  tensor(1.)\n",
            "epoch :  0 time_idx :  310 loss_reg :  tensor(0.6884, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(-0.0583) precision :  tensor(0.1818)\n",
            "epoch :  0 time_idx :  320 loss_reg :  tensor(0.6982, grad_fn=<AddBackward0>) acc :  tensor(0.3765) mcc :  tensor(-0.1228) precision :  tensor(0.3478)\n",
            "epoch :  0 time_idx :  330 loss_reg :  tensor(0.7057, grad_fn=<AddBackward0>) acc :  tensor(0.1647) mcc :  tensor(-0.3422) precision :  tensor(0.1316)\n",
            "epoch :  0 time_idx :  340 loss_reg :  tensor(0.7052, grad_fn=<AddBackward0>) acc :  tensor(0.1647) mcc :  tensor(0.0860) precision :  tensor(0.1125)\n",
            "epoch :  0 time_idx :  350 loss_reg :  tensor(0.6865, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(nan) precision :  tensor(0.)\n",
            "epoch :  0 time_idx :  360 loss_reg :  tensor(0.6966, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(nan) precision :  tensor(0.)\n",
            "epoch :  0 time_idx :  370 loss_reg :  tensor(0.6318, grad_fn=<AddBackward0>) acc :  tensor(0.9412) mcc :  tensor(nan) precision :  tensor(0.)\n",
            "epoch :  0 time_idx :  380 loss_reg :  tensor(0.6876, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.1336) precision :  tensor(1.)\n",
            "Epochs :  0 Iterations :  388 Loss :  0.7447560672293004 ACC :  0.48480898753221424 MCC :  nan Precision :  0.4496090203192392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HiKpYx_YSqi",
        "outputId": "1ad372a5-eda4-4f25-da84-35971ce9f7ee"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  0 time_idx :  0 loss_reg :  tensor(1.1775, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(0.1005)\n",
            "epoch :  0 time_idx :  10 loss_reg :  tensor(0.8964, grad_fn=<AddBackward0>) acc :  tensor(0.6941) mcc :  tensor(0.1277)\n",
            "epoch :  0 time_idx :  20 loss_reg :  tensor(0.8335, grad_fn=<AddBackward0>) acc :  tensor(0.7294) mcc :  tensor(0.0440)\n",
            "epoch :  0 time_idx :  30 loss_reg :  tensor(1.1583, grad_fn=<AddBackward0>) acc :  tensor(0.1176) mcc :  tensor(-0.2830)\n",
            "epoch :  0 time_idx :  40 loss_reg :  tensor(0.8001, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(0.0022)\n",
            "epoch :  0 time_idx :  50 loss_reg :  tensor(0.8670, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.1457)\n",
            "epoch :  0 time_idx :  60 loss_reg :  tensor(0.8143, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(-0.0628)\n",
            "epoch :  0 time_idx :  70 loss_reg :  tensor(0.7893, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(-0.0966)\n",
            "epoch :  0 time_idx :  80 loss_reg :  tensor(0.7794, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(-0.0165)\n",
            "epoch :  0 time_idx :  90 loss_reg :  tensor(0.7644, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.0915)\n",
            "epoch :  0 time_idx :  100 loss_reg :  tensor(0.8248, grad_fn=<AddBackward0>) acc :  tensor(0.2353) mcc :  tensor(0.0152)\n",
            "epoch :  0 time_idx :  110 loss_reg :  tensor(0.7157, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(0.0870)\n",
            "epoch :  0 time_idx :  120 loss_reg :  tensor(0.7531, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(0.0277)\n",
            "epoch :  0 time_idx :  130 loss_reg :  tensor(0.7158, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(-0.0936)\n",
            "epoch :  0 time_idx :  140 loss_reg :  tensor(0.7079, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(-0.0569)\n",
            "epoch :  0 time_idx :  150 loss_reg :  tensor(0.7290, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.0916)\n",
            "epoch :  0 time_idx :  160 loss_reg :  tensor(0.6901, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(0.2091)\n",
            "epoch :  0 time_idx :  170 loss_reg :  tensor(0.7132, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0768)\n",
            "epoch :  0 time_idx :  180 loss_reg :  tensor(0.7022, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(-0.1495)\n",
            "epoch :  0 time_idx :  190 loss_reg :  tensor(0.7560, grad_fn=<AddBackward0>) acc :  tensor(0.1059) mcc :  tensor(0.0352)\n",
            "epoch :  0 time_idx :  200 loss_reg :  tensor(0.7121, grad_fn=<AddBackward0>) acc :  tensor(0.3412) mcc :  tensor(-0.0397)\n",
            "epoch :  0 time_idx :  210 loss_reg :  tensor(0.7175, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(nan)\n",
            "epoch :  0 time_idx :  220 loss_reg :  tensor(0.7529, grad_fn=<AddBackward0>) acc :  tensor(0.2706) mcc :  tensor(nan)\n",
            "epoch :  0 time_idx :  230 loss_reg :  tensor(0.7425, grad_fn=<AddBackward0>) acc :  tensor(0.1765) mcc :  tensor(nan)\n",
            "epoch :  0 time_idx :  240 loss_reg :  tensor(0.7168, grad_fn=<AddBackward0>) acc :  tensor(0.2235) mcc :  tensor(0.1257)\n",
            "epoch :  0 time_idx :  250 loss_reg :  tensor(0.6858, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(-0.0765)\n",
            "epoch :  0 time_idx :  260 loss_reg :  tensor(0.7164, grad_fn=<AddBackward0>) acc :  tensor(0.2000) mcc :  tensor(0.0849)\n",
            "epoch :  0 time_idx :  270 loss_reg :  tensor(0.7004, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.0456)\n",
            "epoch :  0 time_idx :  280 loss_reg :  tensor(0.7110, grad_fn=<AddBackward0>) acc :  tensor(0.3294) mcc :  tensor(-0.2524)\n",
            "epoch :  0 time_idx :  290 loss_reg :  tensor(0.6955, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(-0.0254)\n",
            "epoch :  0 time_idx :  300 loss_reg :  tensor(0.6994, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.1336)\n",
            "epoch :  0 time_idx :  310 loss_reg :  tensor(0.6872, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(-0.0514)\n",
            "epoch :  0 time_idx :  320 loss_reg :  tensor(0.6994, grad_fn=<AddBackward0>) acc :  tensor(0.3412) mcc :  tensor(-0.2185)\n",
            "epoch :  0 time_idx :  330 loss_reg :  tensor(0.7053, grad_fn=<AddBackward0>) acc :  tensor(0.1529) mcc :  tensor(-0.3343)\n",
            "epoch :  0 time_idx :  340 loss_reg :  tensor(0.7048, grad_fn=<AddBackward0>) acc :  tensor(0.1765) mcc :  tensor(0.0948)\n",
            "epoch :  0 time_idx :  350 loss_reg :  tensor(0.6876, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(nan)\n",
            "epoch :  0 time_idx :  360 loss_reg :  tensor(0.6976, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(nan)\n",
            "epoch :  0 time_idx :  370 loss_reg :  tensor(0.6346, grad_fn=<AddBackward0>) acc :  tensor(0.9412) mcc :  tensor(nan)\n",
            "epoch :  0 time_idx :  380 loss_reg :  tensor(0.6894, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(nan)\n",
            "Epochs :  0 Iterations :  388 Loss :  0.7502044333317845\n",
            "epoch :  1 time_idx :  0 loss_reg :  tensor(0.6917, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1686)\n",
            "epoch :  1 time_idx :  10 loss_reg :  tensor(0.6834, grad_fn=<AddBackward0>) acc :  tensor(0.7412) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  20 loss_reg :  tensor(0.7042, grad_fn=<AddBackward0>) acc :  tensor(0.1765) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  30 loss_reg :  tensor(0.7272, grad_fn=<AddBackward0>) acc :  tensor(0.1294) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  40 loss_reg :  tensor(0.6519, grad_fn=<AddBackward0>) acc :  tensor(0.9059) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  50 loss_reg :  tensor(0.6675, grad_fn=<AddBackward0>) acc :  tensor(0.7882) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  60 loss_reg :  tensor(0.6615, grad_fn=<AddBackward0>) acc :  tensor(0.8588) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  70 loss_reg :  tensor(0.6816, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  80 loss_reg :  tensor(0.6895, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  90 loss_reg :  tensor(0.7009, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  100 loss_reg :  tensor(0.7455, grad_fn=<AddBackward0>) acc :  tensor(0.1882) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  110 loss_reg :  tensor(0.6670, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  120 loss_reg :  tensor(0.7198, grad_fn=<AddBackward0>) acc :  tensor(0.3059) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  130 loss_reg :  tensor(0.6826, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  140 loss_reg :  tensor(0.6908, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(-0.0156)\n",
            "epoch :  1 time_idx :  150 loss_reg :  tensor(0.6966, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.1231)\n",
            "epoch :  1 time_idx :  160 loss_reg :  tensor(0.6758, grad_fn=<AddBackward0>) acc :  tensor(0.7176) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  170 loss_reg :  tensor(0.6928, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  180 loss_reg :  tensor(0.6818, grad_fn=<AddBackward0>) acc :  tensor(0.8941) mcc :  tensor(0.5230)\n",
            "epoch :  1 time_idx :  190 loss_reg :  tensor(0.7285, grad_fn=<AddBackward0>) acc :  tensor(0.0941) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  200 loss_reg :  tensor(0.6994, grad_fn=<AddBackward0>) acc :  tensor(0.2118) mcc :  tensor(0.0150)\n",
            "epoch :  1 time_idx :  210 loss_reg :  tensor(0.7031, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  220 loss_reg :  tensor(0.7351, grad_fn=<AddBackward0>) acc :  tensor(0.2706) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  230 loss_reg :  tensor(0.7329, grad_fn=<AddBackward0>) acc :  tensor(0.1647) mcc :  tensor(-0.2357)\n",
            "epoch :  1 time_idx :  240 loss_reg :  tensor(0.7194, grad_fn=<AddBackward0>) acc :  tensor(0.1294) mcc :  tensor(0.0398)\n",
            "epoch :  1 time_idx :  250 loss_reg :  tensor(0.6899, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2013)\n",
            "epoch :  1 time_idx :  260 loss_reg :  tensor(0.7006, grad_fn=<AddBackward0>) acc :  tensor(0.2471) mcc :  tensor(-0.0178)\n",
            "epoch :  1 time_idx :  270 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(-0.1686)\n",
            "epoch :  1 time_idx :  280 loss_reg :  tensor(0.7023, grad_fn=<AddBackward0>) acc :  tensor(0.3529) mcc :  tensor(-0.1440)\n",
            "epoch :  1 time_idx :  290 loss_reg :  tensor(0.6943, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(0.0521)\n",
            "epoch :  1 time_idx :  300 loss_reg :  tensor(0.6947, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.0563)\n",
            "epoch :  1 time_idx :  310 loss_reg :  tensor(0.6893, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(0.0022)\n",
            "epoch :  1 time_idx :  320 loss_reg :  tensor(0.6958, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.1280)\n",
            "epoch :  1 time_idx :  330 loss_reg :  tensor(0.6989, grad_fn=<AddBackward0>) acc :  tensor(0.2471) mcc :  tensor(-0.0782)\n",
            "epoch :  1 time_idx :  340 loss_reg :  tensor(0.6998, grad_fn=<AddBackward0>) acc :  tensor(0.2353) mcc :  tensor(0.1327)\n",
            "epoch :  1 time_idx :  350 loss_reg :  tensor(0.6897, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(-0.1727)\n",
            "epoch :  1 time_idx :  360 loss_reg :  tensor(0.6937, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  370 loss_reg :  tensor(0.6599, grad_fn=<AddBackward0>) acc :  tensor(0.9412) mcc :  tensor(nan)\n",
            "epoch :  1 time_idx :  380 loss_reg :  tensor(0.6868, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2897)\n",
            "epoch :  2 time_idx :  0 loss_reg :  tensor(0.6909, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1006)\n",
            "epoch :  2 time_idx :  10 loss_reg :  tensor(0.6812, grad_fn=<AddBackward0>) acc :  tensor(0.7529) mcc :  tensor(0.2138)\n",
            "epoch :  2 time_idx :  20 loss_reg :  tensor(0.7034, grad_fn=<AddBackward0>) acc :  tensor(0.2824) mcc :  tensor(0.0252)\n",
            "epoch :  2 time_idx :  30 loss_reg :  tensor(0.7078, grad_fn=<AddBackward0>) acc :  tensor(0.2824) mcc :  tensor(-0.0455)\n",
            "epoch :  2 time_idx :  40 loss_reg :  tensor(0.6720, grad_fn=<AddBackward0>) acc :  tensor(0.8000) mcc :  tensor(0.2417)\n",
            "epoch :  2 time_idx :  50 loss_reg :  tensor(0.6798, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(-0.0966)\n",
            "epoch :  2 time_idx :  60 loss_reg :  tensor(0.6734, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(0.0028)\n",
            "epoch :  2 time_idx :  70 loss_reg :  tensor(0.6853, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(-0.0702)\n",
            "epoch :  2 time_idx :  80 loss_reg :  tensor(0.6917, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0432)\n",
            "epoch :  2 time_idx :  90 loss_reg :  tensor(0.6974, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1111)\n",
            "epoch :  2 time_idx :  100 loss_reg :  tensor(0.7324, grad_fn=<AddBackward0>) acc :  tensor(0.2471) mcc :  tensor(0.1204)\n",
            "epoch :  2 time_idx :  110 loss_reg :  tensor(0.6689, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(0.2128)\n",
            "epoch :  2 time_idx :  120 loss_reg :  tensor(0.7082, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(0.1660)\n",
            "epoch :  2 time_idx :  130 loss_reg :  tensor(0.6822, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(-0.0180)\n",
            "epoch :  2 time_idx :  140 loss_reg :  tensor(0.6949, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.0928)\n",
            "epoch :  2 time_idx :  150 loss_reg :  tensor(0.6944, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(-0.1163)\n",
            "epoch :  2 time_idx :  160 loss_reg :  tensor(0.6733, grad_fn=<AddBackward0>) acc :  tensor(0.6941) mcc :  tensor(0.1442)\n",
            "epoch :  2 time_idx :  170 loss_reg :  tensor(0.6913, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(0.0254)\n",
            "epoch :  2 time_idx :  180 loss_reg :  tensor(0.6778, grad_fn=<AddBackward0>) acc :  tensor(0.7059) mcc :  tensor(0.1691)\n",
            "epoch :  2 time_idx :  190 loss_reg :  tensor(0.7007, grad_fn=<AddBackward0>) acc :  tensor(0.3176) mcc :  tensor(0.0149)\n",
            "epoch :  2 time_idx :  200 loss_reg :  tensor(0.6947, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(-0.1809)\n",
            "epoch :  2 time_idx :  210 loss_reg :  tensor(0.7012, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.1889)\n",
            "epoch :  2 time_idx :  220 loss_reg :  tensor(0.7069, grad_fn=<AddBackward0>) acc :  tensor(0.3529) mcc :  tensor(0.0381)\n",
            "epoch :  2 time_idx :  230 loss_reg :  tensor(0.7149, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.0862)\n",
            "epoch :  2 time_idx :  240 loss_reg :  tensor(0.7089, grad_fn=<AddBackward0>) acc :  tensor(0.3412) mcc :  tensor(-0.1785)\n",
            "epoch :  2 time_idx :  250 loss_reg :  tensor(0.6874, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.2502)\n",
            "epoch :  2 time_idx :  260 loss_reg :  tensor(0.6899, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.1441)\n",
            "epoch :  2 time_idx :  270 loss_reg :  tensor(0.6946, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.2094)\n",
            "epoch :  2 time_idx :  280 loss_reg :  tensor(0.6963, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(0.0338)\n",
            "epoch :  2 time_idx :  290 loss_reg :  tensor(0.6929, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1598)\n",
            "epoch :  2 time_idx :  300 loss_reg :  tensor(0.6930, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.0203)\n",
            "epoch :  2 time_idx :  310 loss_reg :  tensor(0.6937, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(-0.0780)\n",
            "epoch :  2 time_idx :  320 loss_reg :  tensor(0.6947, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.1129)\n",
            "epoch :  2 time_idx :  330 loss_reg :  tensor(0.6951, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1311)\n",
            "epoch :  2 time_idx :  340 loss_reg :  tensor(0.6946, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.0146)\n",
            "epoch :  2 time_idx :  350 loss_reg :  tensor(0.6911, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0220)\n",
            "epoch :  2 time_idx :  360 loss_reg :  tensor(0.6965, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.1084)\n",
            "epoch :  2 time_idx :  370 loss_reg :  tensor(0.6787, grad_fn=<AddBackward0>) acc :  tensor(0.7765) mcc :  tensor(0.1152)\n",
            "epoch :  2 time_idx :  380 loss_reg :  tensor(0.6842, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2406)\n",
            "epoch :  3 time_idx :  0 loss_reg :  tensor(0.6874, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1631)\n",
            "epoch :  3 time_idx :  10 loss_reg :  tensor(0.6808, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2881)\n",
            "epoch :  3 time_idx :  20 loss_reg :  tensor(0.6960, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.0582)\n",
            "epoch :  3 time_idx :  30 loss_reg :  tensor(0.7006, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.0150)\n",
            "epoch :  3 time_idx :  40 loss_reg :  tensor(0.6821, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.1314)\n",
            "epoch :  3 time_idx :  50 loss_reg :  tensor(0.6832, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1322)\n",
            "epoch :  3 time_idx :  60 loss_reg :  tensor(0.6847, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.0237)\n",
            "epoch :  3 time_idx :  70 loss_reg :  tensor(0.6896, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(-0.0573)\n",
            "epoch :  3 time_idx :  80 loss_reg :  tensor(0.6965, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(-0.0544)\n",
            "epoch :  3 time_idx :  90 loss_reg :  tensor(0.6949, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(0.0092)\n",
            "epoch :  3 time_idx :  100 loss_reg :  tensor(0.7231, grad_fn=<AddBackward0>) acc :  tensor(0.3294) mcc :  tensor(0.1327)\n",
            "epoch :  3 time_idx :  110 loss_reg :  tensor(0.6702, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(0.1588)\n",
            "epoch :  3 time_idx :  120 loss_reg :  tensor(0.7017, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(0.2403)\n",
            "epoch :  3 time_idx :  130 loss_reg :  tensor(0.6844, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(-0.0302)\n",
            "epoch :  3 time_idx :  140 loss_reg :  tensor(0.6923, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0077)\n",
            "epoch :  3 time_idx :  150 loss_reg :  tensor(0.6966, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.2056)\n",
            "epoch :  3 time_idx :  160 loss_reg :  tensor(0.6762, grad_fn=<AddBackward0>) acc :  tensor(0.6941) mcc :  tensor(0.1866)\n",
            "epoch :  3 time_idx :  170 loss_reg :  tensor(0.6867, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1454)\n",
            "epoch :  3 time_idx :  180 loss_reg :  tensor(0.6768, grad_fn=<AddBackward0>) acc :  tensor(0.6471) mcc :  tensor(0.1079)\n",
            "epoch :  3 time_idx :  190 loss_reg :  tensor(0.6986, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.1499)\n",
            "epoch :  3 time_idx :  200 loss_reg :  tensor(0.6967, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.1650)\n",
            "epoch :  3 time_idx :  210 loss_reg :  tensor(0.7018, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1294)\n",
            "epoch :  3 time_idx :  220 loss_reg :  tensor(0.6997, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(0.1852)\n",
            "epoch :  3 time_idx :  230 loss_reg :  tensor(0.7114, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.1143)\n",
            "epoch :  3 time_idx :  240 loss_reg :  tensor(0.7126, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.2139)\n",
            "epoch :  3 time_idx :  250 loss_reg :  tensor(0.6855, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2619)\n",
            "epoch :  3 time_idx :  260 loss_reg :  tensor(0.6832, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2387)\n",
            "epoch :  3 time_idx :  270 loss_reg :  tensor(0.6964, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(-0.2225)\n",
            "epoch :  3 time_idx :  280 loss_reg :  tensor(0.6984, grad_fn=<AddBackward0>) acc :  tensor(0.3412) mcc :  tensor(-0.2396)\n",
            "epoch :  3 time_idx :  290 loss_reg :  tensor(0.6940, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.1166)\n",
            "epoch :  3 time_idx :  300 loss_reg :  tensor(0.6915, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1925)\n",
            "epoch :  3 time_idx :  310 loss_reg :  tensor(0.6945, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1294)\n",
            "epoch :  3 time_idx :  320 loss_reg :  tensor(0.6951, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.0900)\n",
            "epoch :  3 time_idx :  330 loss_reg :  tensor(0.6954, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.1102)\n",
            "epoch :  3 time_idx :  340 loss_reg :  tensor(0.6957, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.0397)\n",
            "epoch :  3 time_idx :  350 loss_reg :  tensor(0.6902, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.0406)\n",
            "epoch :  3 time_idx :  360 loss_reg :  tensor(0.6952, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.2266)\n",
            "epoch :  3 time_idx :  370 loss_reg :  tensor(0.6820, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.0442)\n",
            "epoch :  3 time_idx :  380 loss_reg :  tensor(0.6829, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3266)\n",
            "epoch :  4 time_idx :  0 loss_reg :  tensor(0.6834, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2273)\n",
            "epoch :  4 time_idx :  10 loss_reg :  tensor(0.6789, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.3017)\n",
            "epoch :  4 time_idx :  20 loss_reg :  tensor(0.6978, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(0.0655)\n",
            "epoch :  4 time_idx :  30 loss_reg :  tensor(0.7022, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(0.0124)\n",
            "epoch :  4 time_idx :  40 loss_reg :  tensor(0.6824, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1233)\n",
            "epoch :  4 time_idx :  50 loss_reg :  tensor(0.6857, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.1457)\n",
            "epoch :  4 time_idx :  60 loss_reg :  tensor(0.6840, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.0438)\n",
            "epoch :  4 time_idx :  70 loss_reg :  tensor(0.6896, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.0580)\n",
            "epoch :  4 time_idx :  80 loss_reg :  tensor(0.6928, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0291)\n",
            "epoch :  4 time_idx :  90 loss_reg :  tensor(0.6942, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(0.0204)\n",
            "epoch :  4 time_idx :  100 loss_reg :  tensor(0.7124, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(0.1037)\n",
            "epoch :  4 time_idx :  110 loss_reg :  tensor(0.6759, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1486)\n",
            "epoch :  4 time_idx :  120 loss_reg :  tensor(0.6917, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.3566)\n",
            "epoch :  4 time_idx :  130 loss_reg :  tensor(0.6865, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.0545)\n",
            "epoch :  4 time_idx :  140 loss_reg :  tensor(0.6943, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.0243)\n",
            "epoch :  4 time_idx :  150 loss_reg :  tensor(0.6966, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.1366)\n",
            "epoch :  4 time_idx :  160 loss_reg :  tensor(0.6823, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2277)\n",
            "epoch :  4 time_idx :  170 loss_reg :  tensor(0.6814, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2111)\n",
            "epoch :  4 time_idx :  180 loss_reg :  tensor(0.6803, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.0534)\n",
            "epoch :  4 time_idx :  190 loss_reg :  tensor(0.6971, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.1650)\n",
            "epoch :  4 time_idx :  200 loss_reg :  tensor(0.6980, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.1574)\n",
            "epoch :  4 time_idx :  210 loss_reg :  tensor(0.7068, grad_fn=<AddBackward0>) acc :  tensor(0.3529) mcc :  tensor(-0.2850)\n",
            "epoch :  4 time_idx :  220 loss_reg :  tensor(0.6979, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(0.1861)\n",
            "epoch :  4 time_idx :  230 loss_reg :  tensor(0.7124, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.1029)\n",
            "epoch :  4 time_idx :  240 loss_reg :  tensor(0.7112, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.1858)\n",
            "epoch :  4 time_idx :  250 loss_reg :  tensor(0.6847, grad_fn=<AddBackward0>) acc :  tensor(0.6471) mcc :  tensor(0.3259)\n",
            "epoch :  4 time_idx :  260 loss_reg :  tensor(0.6829, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2168)\n",
            "epoch :  4 time_idx :  270 loss_reg :  tensor(0.6979, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.2520)\n",
            "epoch :  4 time_idx :  280 loss_reg :  tensor(0.6981, grad_fn=<AddBackward0>) acc :  tensor(0.3765) mcc :  tensor(-0.1988)\n",
            "epoch :  4 time_idx :  290 loss_reg :  tensor(0.6918, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0871)\n",
            "epoch :  4 time_idx :  300 loss_reg :  tensor(0.6911, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.1063)\n",
            "epoch :  4 time_idx :  310 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.1431)\n",
            "epoch :  4 time_idx :  320 loss_reg :  tensor(0.6954, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.0760)\n",
            "epoch :  4 time_idx :  330 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.0980)\n",
            "epoch :  4 time_idx :  340 loss_reg :  tensor(0.6944, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.0751)\n",
            "epoch :  4 time_idx :  350 loss_reg :  tensor(0.6909, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1014)\n",
            "epoch :  4 time_idx :  360 loss_reg :  tensor(0.6954, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.0816)\n",
            "epoch :  4 time_idx :  370 loss_reg :  tensor(0.6831, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1086)\n",
            "epoch :  4 time_idx :  380 loss_reg :  tensor(0.6795, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2498)\n",
            "epoch :  5 time_idx :  0 loss_reg :  tensor(0.6808, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2451)\n",
            "epoch :  5 time_idx :  10 loss_reg :  tensor(0.6748, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1566)\n",
            "epoch :  5 time_idx :  20 loss_reg :  tensor(0.6959, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.1891)\n",
            "epoch :  5 time_idx :  30 loss_reg :  tensor(0.7023, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.0857)\n",
            "epoch :  5 time_idx :  40 loss_reg :  tensor(0.6815, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.1397)\n",
            "epoch :  5 time_idx :  50 loss_reg :  tensor(0.6854, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1322)\n",
            "epoch :  5 time_idx :  60 loss_reg :  tensor(0.6849, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.1140)\n",
            "epoch :  5 time_idx :  70 loss_reg :  tensor(0.6888, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.0683)\n",
            "epoch :  5 time_idx :  80 loss_reg :  tensor(0.6948, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0069)\n",
            "epoch :  5 time_idx :  90 loss_reg :  tensor(0.6961, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(-0.0069)\n",
            "epoch :  5 time_idx :  100 loss_reg :  tensor(0.7102, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(0.1366)\n",
            "epoch :  5 time_idx :  110 loss_reg :  tensor(0.6788, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.0955)\n",
            "epoch :  5 time_idx :  120 loss_reg :  tensor(0.6922, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.3164)\n",
            "epoch :  5 time_idx :  130 loss_reg :  tensor(0.6882, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(-0.0169)\n",
            "epoch :  5 time_idx :  140 loss_reg :  tensor(0.6940, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(-0.0088)\n",
            "epoch :  5 time_idx :  150 loss_reg :  tensor(0.6979, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.1957)\n",
            "epoch :  5 time_idx :  160 loss_reg :  tensor(0.6821, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2477)\n",
            "epoch :  5 time_idx :  170 loss_reg :  tensor(0.6777, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2306)\n",
            "epoch :  5 time_idx :  180 loss_reg :  tensor(0.6817, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.0430)\n",
            "epoch :  5 time_idx :  190 loss_reg :  tensor(0.6958, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1804)\n",
            "epoch :  5 time_idx :  200 loss_reg :  tensor(0.7016, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(-0.1351)\n",
            "epoch :  5 time_idx :  210 loss_reg :  tensor(0.7086, grad_fn=<AddBackward0>) acc :  tensor(0.2824) mcc :  tensor(-0.4292)\n",
            "epoch :  5 time_idx :  220 loss_reg :  tensor(0.6981, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(0.0564)\n",
            "epoch :  5 time_idx :  230 loss_reg :  tensor(0.7106, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.1143)\n",
            "epoch :  5 time_idx :  240 loss_reg :  tensor(0.7102, grad_fn=<AddBackward0>) acc :  tensor(0.4000) mcc :  tensor(-0.1858)\n",
            "epoch :  5 time_idx :  250 loss_reg :  tensor(0.6824, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2778)\n",
            "epoch :  5 time_idx :  260 loss_reg :  tensor(0.6825, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.2061)\n",
            "epoch :  5 time_idx :  270 loss_reg :  tensor(0.6985, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.2094)\n",
            "epoch :  5 time_idx :  280 loss_reg :  tensor(0.6974, grad_fn=<AddBackward0>) acc :  tensor(0.3765) mcc :  tensor(-0.1765)\n",
            "epoch :  5 time_idx :  290 loss_reg :  tensor(0.6912, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.0803)\n",
            "epoch :  5 time_idx :  300 loss_reg :  tensor(0.6924, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1841)\n",
            "epoch :  5 time_idx :  310 loss_reg :  tensor(0.6926, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(-0.0482)\n",
            "epoch :  5 time_idx :  320 loss_reg :  tensor(0.6956, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1090)\n",
            "epoch :  5 time_idx :  330 loss_reg :  tensor(0.6939, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(-0.0254)\n",
            "epoch :  5 time_idx :  340 loss_reg :  tensor(0.6956, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.0668)\n",
            "epoch :  5 time_idx :  350 loss_reg :  tensor(0.6906, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1203)\n",
            "epoch :  5 time_idx :  360 loss_reg :  tensor(0.6954, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1548)\n",
            "epoch :  5 time_idx :  370 loss_reg :  tensor(0.6835, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.1021)\n",
            "epoch :  5 time_idx :  380 loss_reg :  tensor(0.6790, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3368)\n",
            "epoch :  6 time_idx :  0 loss_reg :  tensor(0.6774, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3266)\n",
            "epoch :  6 time_idx :  10 loss_reg :  tensor(0.6729, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.4092)\n",
            "epoch :  6 time_idx :  20 loss_reg :  tensor(0.6961, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1598)\n",
            "epoch :  6 time_idx :  30 loss_reg :  tensor(0.7044, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.0857)\n",
            "epoch :  6 time_idx :  40 loss_reg :  tensor(0.6824, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2046)\n",
            "epoch :  6 time_idx :  50 loss_reg :  tensor(0.6862, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.0598)\n",
            "epoch :  6 time_idx :  60 loss_reg :  tensor(0.6840, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1034)\n",
            "epoch :  6 time_idx :  70 loss_reg :  tensor(0.6907, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1189)\n",
            "epoch :  6 time_idx :  80 loss_reg :  tensor(0.6946, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0069)\n",
            "epoch :  6 time_idx :  90 loss_reg :  tensor(0.6935, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0638)\n",
            "epoch :  6 time_idx :  100 loss_reg :  tensor(0.7094, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(0.0926)\n",
            "epoch :  6 time_idx :  110 loss_reg :  tensor(0.6823, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2101)\n",
            "epoch :  6 time_idx :  120 loss_reg :  tensor(0.6967, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(0.2816)\n",
            "epoch :  6 time_idx :  130 loss_reg :  tensor(0.6891, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(-0.1148)\n",
            "epoch :  6 time_idx :  140 loss_reg :  tensor(0.6973, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.1257)\n",
            "epoch :  6 time_idx :  150 loss_reg :  tensor(0.6978, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.1194)\n",
            "epoch :  6 time_idx :  160 loss_reg :  tensor(0.6824, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.1550)\n",
            "epoch :  6 time_idx :  170 loss_reg :  tensor(0.6768, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2562)\n",
            "epoch :  6 time_idx :  180 loss_reg :  tensor(0.6857, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0679)\n",
            "epoch :  6 time_idx :  190 loss_reg :  tensor(0.7032, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(0.1878)\n",
            "epoch :  6 time_idx :  200 loss_reg :  tensor(0.7081, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.3186)\n",
            "epoch :  6 time_idx :  210 loss_reg :  tensor(0.7057, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.2092)\n",
            "epoch :  6 time_idx :  220 loss_reg :  tensor(0.7005, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(0.1277)\n",
            "epoch :  6 time_idx :  230 loss_reg :  tensor(0.7080, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.0630)\n",
            "epoch :  6 time_idx :  240 loss_reg :  tensor(0.7072, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.0565)\n",
            "epoch :  6 time_idx :  250 loss_reg :  tensor(0.6823, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2257)\n",
            "epoch :  6 time_idx :  260 loss_reg :  tensor(0.6817, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2168)\n",
            "epoch :  6 time_idx :  270 loss_reg :  tensor(0.6980, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.2197)\n",
            "epoch :  6 time_idx :  280 loss_reg :  tensor(0.6983, grad_fn=<AddBackward0>) acc :  tensor(0.3294) mcc :  tensor(-0.2796)\n",
            "epoch :  6 time_idx :  290 loss_reg :  tensor(0.6924, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(-0.0546)\n",
            "epoch :  6 time_idx :  300 loss_reg :  tensor(0.6924, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0872)\n",
            "epoch :  6 time_idx :  310 loss_reg :  tensor(0.6920, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(-0.0078)\n",
            "epoch :  6 time_idx :  320 loss_reg :  tensor(0.6926, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.1183)\n",
            "epoch :  6 time_idx :  330 loss_reg :  tensor(0.6926, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(0.0655)\n",
            "epoch :  6 time_idx :  340 loss_reg :  tensor(0.6928, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0064)\n",
            "epoch :  6 time_idx :  350 loss_reg :  tensor(0.6899, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.0867)\n",
            "epoch :  6 time_idx :  360 loss_reg :  tensor(0.6980, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.0579)\n",
            "epoch :  6 time_idx :  370 loss_reg :  tensor(0.6790, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(0.0511)\n",
            "epoch :  6 time_idx :  380 loss_reg :  tensor(0.6790, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3172)\n",
            "epoch :  7 time_idx :  0 loss_reg :  tensor(0.6845, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2451)\n",
            "epoch :  7 time_idx :  10 loss_reg :  tensor(0.6757, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.3419)\n",
            "epoch :  7 time_idx :  20 loss_reg :  tensor(0.6949, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2325)\n",
            "epoch :  7 time_idx :  30 loss_reg :  tensor(0.7019, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.0951)\n",
            "epoch :  7 time_idx :  40 loss_reg :  tensor(0.6848, grad_fn=<AddBackward0>) acc :  tensor(0.6471) mcc :  tensor(0.2303)\n",
            "epoch :  7 time_idx :  50 loss_reg :  tensor(0.6857, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.0598)\n",
            "epoch :  7 time_idx :  60 loss_reg :  tensor(0.6868, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.0828)\n",
            "epoch :  7 time_idx :  70 loss_reg :  tensor(0.6908, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1380)\n",
            "epoch :  7 time_idx :  80 loss_reg :  tensor(0.6926, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0386)\n",
            "epoch :  7 time_idx :  90 loss_reg :  tensor(0.6942, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1482)\n",
            "epoch :  7 time_idx :  100 loss_reg :  tensor(0.7075, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(0.1773)\n",
            "epoch :  7 time_idx :  110 loss_reg :  tensor(0.6803, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.1503)\n",
            "epoch :  7 time_idx :  120 loss_reg :  tensor(0.6957, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(0.2336)\n",
            "epoch :  7 time_idx :  130 loss_reg :  tensor(0.6892, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(-0.0522)\n",
            "epoch :  7 time_idx :  140 loss_reg :  tensor(0.6965, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.0579)\n",
            "epoch :  7 time_idx :  150 loss_reg :  tensor(0.6988, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.1519)\n",
            "epoch :  7 time_idx :  160 loss_reg :  tensor(0.6828, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.0773)\n",
            "epoch :  7 time_idx :  170 loss_reg :  tensor(0.6789, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3360)\n",
            "epoch :  7 time_idx :  180 loss_reg :  tensor(0.6821, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0679)\n",
            "epoch :  7 time_idx :  190 loss_reg :  tensor(0.7028, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(0.1278)\n",
            "epoch :  7 time_idx :  200 loss_reg :  tensor(0.7083, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.3501)\n",
            "epoch :  7 time_idx :  210 loss_reg :  tensor(0.7075, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.2620)\n",
            "epoch :  7 time_idx :  220 loss_reg :  tensor(0.6988, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(0.1785)\n",
            "epoch :  7 time_idx :  230 loss_reg :  tensor(0.7104, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.0630)\n",
            "epoch :  7 time_idx :  240 loss_reg :  tensor(0.7085, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.1397)\n",
            "epoch :  7 time_idx :  250 loss_reg :  tensor(0.6839, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.3097)\n",
            "epoch :  7 time_idx :  260 loss_reg :  tensor(0.6824, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.3754)\n",
            "epoch :  7 time_idx :  270 loss_reg :  tensor(0.6984, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.2510)\n",
            "epoch :  7 time_idx :  280 loss_reg :  tensor(0.6982, grad_fn=<AddBackward0>) acc :  tensor(0.3412) mcc :  tensor(-0.2587)\n",
            "epoch :  7 time_idx :  290 loss_reg :  tensor(0.6919, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.0582)\n",
            "epoch :  7 time_idx :  300 loss_reg :  tensor(0.6932, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.1345)\n",
            "epoch :  7 time_idx :  310 loss_reg :  tensor(0.6932, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.0751)\n",
            "epoch :  7 time_idx :  320 loss_reg :  tensor(0.6946, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.1002)\n",
            "epoch :  7 time_idx :  330 loss_reg :  tensor(0.6940, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.0293)\n",
            "epoch :  7 time_idx :  340 loss_reg :  tensor(0.6945, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(-0.0342)\n",
            "epoch :  7 time_idx :  350 loss_reg :  tensor(0.6896, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1316)\n",
            "epoch :  7 time_idx :  360 loss_reg :  tensor(0.6965, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.1057)\n",
            "epoch :  7 time_idx :  370 loss_reg :  tensor(0.6857, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.0956)\n",
            "epoch :  7 time_idx :  380 loss_reg :  tensor(0.6782, grad_fn=<AddBackward0>) acc :  tensor(0.6824) mcc :  tensor(0.3747)\n",
            "epoch :  8 time_idx :  0 loss_reg :  tensor(0.6809, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2319)\n",
            "epoch :  8 time_idx :  10 loss_reg :  tensor(0.6733, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.2896)\n",
            "epoch :  8 time_idx :  20 loss_reg :  tensor(0.6944, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1707)\n",
            "epoch :  8 time_idx :  30 loss_reg :  tensor(0.7038, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.0429)\n",
            "epoch :  8 time_idx :  40 loss_reg :  tensor(0.6833, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.1314)\n",
            "epoch :  8 time_idx :  50 loss_reg :  tensor(0.6831, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.0992)\n",
            "epoch :  8 time_idx :  60 loss_reg :  tensor(0.6849, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.1140)\n",
            "epoch :  8 time_idx :  70 loss_reg :  tensor(0.6903, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.0887)\n",
            "epoch :  8 time_idx :  80 loss_reg :  tensor(0.6911, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.1282)\n",
            "epoch :  8 time_idx :  90 loss_reg :  tensor(0.6947, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0914)\n",
            "epoch :  8 time_idx :  100 loss_reg :  tensor(0.7062, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(0.1667)\n",
            "epoch :  8 time_idx :  110 loss_reg :  tensor(0.6791, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.2052)\n",
            "epoch :  8 time_idx :  120 loss_reg :  tensor(0.6932, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.3030)\n",
            "epoch :  8 time_idx :  130 loss_reg :  tensor(0.6908, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(-0.0600)\n",
            "epoch :  8 time_idx :  140 loss_reg :  tensor(0.6954, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(-0.0095)\n",
            "epoch :  8 time_idx :  150 loss_reg :  tensor(0.7009, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1564)\n",
            "epoch :  8 time_idx :  160 loss_reg :  tensor(0.6808, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2075)\n",
            "epoch :  8 time_idx :  170 loss_reg :  tensor(0.6798, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3308)\n",
            "epoch :  8 time_idx :  180 loss_reg :  tensor(0.6787, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.1094)\n",
            "epoch :  8 time_idx :  190 loss_reg :  tensor(0.6995, grad_fn=<AddBackward0>) acc :  tensor(0.4941) mcc :  tensor(0.1278)\n",
            "epoch :  8 time_idx :  200 loss_reg :  tensor(0.7080, grad_fn=<AddBackward0>) acc :  tensor(0.3765) mcc :  tensor(-0.3419)\n",
            "epoch :  8 time_idx :  210 loss_reg :  tensor(0.7126, grad_fn=<AddBackward0>) acc :  tensor(0.3765) mcc :  tensor(-0.2419)\n",
            "epoch :  8 time_idx :  220 loss_reg :  tensor(0.6980, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(0.0473)\n",
            "epoch :  8 time_idx :  230 loss_reg :  tensor(0.7086, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.0630)\n",
            "epoch :  8 time_idx :  240 loss_reg :  tensor(0.7100, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1123)\n",
            "epoch :  8 time_idx :  250 loss_reg :  tensor(0.6828, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2758)\n",
            "epoch :  8 time_idx :  260 loss_reg :  tensor(0.6799, grad_fn=<AddBackward0>) acc :  tensor(0.6353) mcc :  tensor(0.3331)\n",
            "epoch :  8 time_idx :  270 loss_reg :  tensor(0.6978, grad_fn=<AddBackward0>) acc :  tensor(0.4588) mcc :  tensor(-0.2619)\n",
            "epoch :  8 time_idx :  280 loss_reg :  tensor(0.6976, grad_fn=<AddBackward0>) acc :  tensor(0.3647) mcc :  tensor(-0.2076)\n",
            "epoch :  8 time_idx :  290 loss_reg :  tensor(0.6916, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0472)\n",
            "epoch :  8 time_idx :  300 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0580)\n",
            "epoch :  8 time_idx :  310 loss_reg :  tensor(0.6931, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(-0.0628)\n",
            "epoch :  8 time_idx :  320 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0339)\n",
            "epoch :  8 time_idx :  330 loss_reg :  tensor(0.6914, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1424)\n",
            "epoch :  8 time_idx :  340 loss_reg :  tensor(0.6942, grad_fn=<AddBackward0>) acc :  tensor(0.5059) mcc :  tensor(0.0342)\n",
            "epoch :  8 time_idx :  350 loss_reg :  tensor(0.6907, grad_fn=<AddBackward0>) acc :  tensor(0.5176) mcc :  tensor(0.0035)\n",
            "epoch :  8 time_idx :  360 loss_reg :  tensor(0.6934, grad_fn=<AddBackward0>) acc :  tensor(0.4824) mcc :  tensor(-0.0337)\n",
            "epoch :  8 time_idx :  370 loss_reg :  tensor(0.6845, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.0183)\n",
            "epoch :  8 time_idx :  380 loss_reg :  tensor(0.6784, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.3368)\n",
            "epoch :  9 time_idx :  0 loss_reg :  tensor(0.6822, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2233)\n",
            "epoch :  9 time_idx :  10 loss_reg :  tensor(0.6713, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3957)\n",
            "epoch :  9 time_idx :  20 loss_reg :  tensor(0.6946, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0293)\n",
            "epoch :  9 time_idx :  30 loss_reg :  tensor(0.7042, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.0242)\n",
            "epoch :  9 time_idx :  40 loss_reg :  tensor(0.6800, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1480)\n",
            "epoch :  9 time_idx :  50 loss_reg :  tensor(0.6841, grad_fn=<AddBackward0>) acc :  tensor(0.5765) mcc :  tensor(0.0133)\n",
            "epoch :  9 time_idx :  60 loss_reg :  tensor(0.6846, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.1842)\n",
            "epoch :  9 time_idx :  70 loss_reg :  tensor(0.6914, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0377)\n",
            "epoch :  9 time_idx :  80 loss_reg :  tensor(0.6909, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0827)\n",
            "epoch :  9 time_idx :  90 loss_reg :  tensor(0.6922, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.1126)\n",
            "epoch :  9 time_idx :  100 loss_reg :  tensor(0.7057, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(0.1667)\n",
            "epoch :  9 time_idx :  110 loss_reg :  tensor(0.6818, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.1486)\n",
            "epoch :  9 time_idx :  120 loss_reg :  tensor(0.6905, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.3023)\n",
            "epoch :  9 time_idx :  130 loss_reg :  tensor(0.6896, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.0805)\n",
            "epoch :  9 time_idx :  140 loss_reg :  tensor(0.6950, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(-0.0754)\n",
            "epoch :  9 time_idx :  150 loss_reg :  tensor(0.7011, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.1564)\n",
            "epoch :  9 time_idx :  160 loss_reg :  tensor(0.6804, grad_fn=<AddBackward0>) acc :  tensor(0.6706) mcc :  tensor(0.2277)\n",
            "epoch :  9 time_idx :  170 loss_reg :  tensor(0.6800, grad_fn=<AddBackward0>) acc :  tensor(0.6235) mcc :  tensor(0.2562)\n",
            "epoch :  9 time_idx :  180 loss_reg :  tensor(0.6771, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0225)\n",
            "epoch :  9 time_idx :  190 loss_reg :  tensor(0.6935, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.0997)\n",
            "epoch :  9 time_idx :  200 loss_reg :  tensor(0.7042, grad_fn=<AddBackward0>) acc :  tensor(0.4353) mcc :  tensor(-0.3039)\n",
            "epoch :  9 time_idx :  210 loss_reg :  tensor(0.7145, grad_fn=<AddBackward0>) acc :  tensor(0.3294) mcc :  tensor(-0.3323)\n",
            "epoch :  9 time_idx :  220 loss_reg :  tensor(0.7000, grad_fn=<AddBackward0>) acc :  tensor(0.4706) mcc :  tensor(0.0253)\n",
            "epoch :  9 time_idx :  230 loss_reg :  tensor(0.7086, grad_fn=<AddBackward0>) acc :  tensor(0.4235) mcc :  tensor(-0.0516)\n",
            "epoch :  9 time_idx :  240 loss_reg :  tensor(0.7100, grad_fn=<AddBackward0>) acc :  tensor(0.4118) mcc :  tensor(-0.1213)\n",
            "epoch :  9 time_idx :  250 loss_reg :  tensor(0.6832, grad_fn=<AddBackward0>) acc :  tensor(0.6000) mcc :  tensor(0.2619)\n",
            "epoch :  9 time_idx :  260 loss_reg :  tensor(0.6793, grad_fn=<AddBackward0>) acc :  tensor(0.6588) mcc :  tensor(0.3138)\n",
            "epoch :  9 time_idx :  270 loss_reg :  tensor(0.6986, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.2300)\n",
            "epoch :  9 time_idx :  280 loss_reg :  tensor(0.6994, grad_fn=<AddBackward0>) acc :  tensor(0.3882) mcc :  tensor(-0.1680)\n",
            "epoch :  9 time_idx :  290 loss_reg :  tensor(0.6912, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.2045)\n",
            "epoch :  9 time_idx :  300 loss_reg :  tensor(0.6951, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.0869)\n",
            "epoch :  9 time_idx :  310 loss_reg :  tensor(0.6916, grad_fn=<AddBackward0>) acc :  tensor(0.5412) mcc :  tensor(0.0611)\n",
            "epoch :  9 time_idx :  320 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.5529) mcc :  tensor(0.1307)\n",
            "epoch :  9 time_idx :  330 loss_reg :  tensor(0.6912, grad_fn=<AddBackward0>) acc :  tensor(0.5882) mcc :  tensor(0.1818)\n",
            "epoch :  9 time_idx :  340 loss_reg :  tensor(0.6938, grad_fn=<AddBackward0>) acc :  tensor(0.4471) mcc :  tensor(-0.0064)\n",
            "epoch :  9 time_idx :  350 loss_reg :  tensor(0.6902, grad_fn=<AddBackward0>) acc :  tensor(0.5647) mcc :  tensor(0.0900)\n",
            "epoch :  9 time_idx :  360 loss_reg :  tensor(0.6897, grad_fn=<AddBackward0>) acc :  tensor(0.5294) mcc :  tensor(0.0631)\n",
            "epoch :  9 time_idx :  370 loss_reg :  tensor(0.6844, grad_fn=<AddBackward0>) acc :  tensor(0.6118) mcc :  tensor(0.0121)\n",
            "epoch :  9 time_idx :  380 loss_reg :  tensor(0.6778, grad_fn=<AddBackward0>) acc :  tensor(0.6941) mcc :  tensor(0.3939)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9zup0EEC-ER"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}